[{"content":"This post is about part C of Lab2 of MIT 6.824, Distributed Systems. For previous two parts, please refer to Part A and B. In this one we are focusing on Persist in Raft. The implementation would mostly follow the figure 2 in the paper as well.\nPersist It\u0026rsquo;s inevitable to have some machines crashed or rebooted in the clusters. We need to persist some important state so that nodes can begin at the place it crashed and rejoin the cluster again.\nAs Persistent state on all servers mentioned in figure 2, there three state to persist:\n* `currentTerm`: This will avoid the node to vote a `candidate` with a smaller `term` * `votedFor`: this prevent a node from voting multiple times in an election * `log[]`: if the node has the majority of committed logs, we need to ensure the future `leader` can see them  Implementation in the Lab Following the comments in function persist() and readPersist() of raft.go, we can fill the functions.\n// // save Raft's persistent state to stable storage, // where it can later be retrieved after a crash and restart. // see paper's Figure 2 for a description of what should be persistent. // // Call it when lock acquried func (rf *Raft) persist() { w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.currTerm) e.Encode(rf.votedFor) e.Encode(rf.log) data := w.Bytes() rf.persister.SaveRaftState(data) } // // restore previously persisted state // func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) \u0026lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var currTerm int var votedFor int var log []LogEntry if d.Decode(\u0026amp;currTerm) != nil || d.Decode(\u0026amp;votedFor) != nil || d.Decode(\u0026amp;log) != nil { panic(\u0026quot;Miss states for node recovery\u0026quot;) } else { rf.currTerm = currTerm rf.votedFor = votedFor rf.log = log DPrintf(\u0026quot;[%v] reboot, Term %v\u0026quot;, rf.me, rf.currTerm) } }  persist() needs to be called once one of the three variables listed above gets updated. And readPersist() should be called when Make() gets called.\nOptimization Some tests still get failed sometimes, below are the optimization on top of implementation following figure 2 to omit the failures.\nLog disagreement Sometimes the test would fail because taking too long to reach an agreement. You can follow the instructions in the An aside on optimizations in the post Students' Guide to Raft mentioned in the handout. Add two extra variables in AppendEntriesRPC reply: ConflictTerm and ConflictIndex to speed up decrement in stead of decreasing one term per round.\nSo at leader side we have:\n// Check if the leader has a log at the ConflitTerm // and get the last entry in the term lastIdx := rf.searchLastEntryInTerm(reply.ConflictTerm) if firstIndex != -1 { // Assume the follower have the all the logs in that term // and some bad logs in term between ConflictTerm and leader's term // have such a term, try to append from the next term rf.nextIndex[i] = firstIndex + 1 } else { // That term doesn't exist, so try to check if can agree on the log at ConflictIndex rf.nextIndex[i] = reply.ConflictIndex }  And at the follower side:\nif args.PreLogIndex \u0026gt; lastIdx { // The expected PreLogIndex \u0026gt; actual last index on follower // Ask for decrement reply.IsSuccess = false reply.ConflictIndex = len(rf.log) reply.ConflictTerm = -1 args.PreLogIndex, lastIdx) } else if rf.log[args.PreLogIndex].Term != args.PreLogTerm { // Unable to reach agreement at the PreLogIndex. Ask for decrement reply.IsSuccess = false reply.ConflictTerm = rf.log[args.PreLogIndex].Term reply.ConflictIndex = rf.searchFirstEntryInTerm(args.PreLogIndex) } else { // Able to reach agreement at the PreLogIndex // ... }  commitIndex Persist An other failure I sometimes met in the test TestFigure82C is the apply error:\nTest (2C): Figure 8 ... 2021/01/06 23:22:04 apply error: commit index=5 server=4 3879934624216030722 != server=3 6543482455276445645 exit status 1  As I checked the log, I see the situation like what happened in figure 8 in paper. Given 3 nodes, s1, s2 and s3:\n  s1, s2 and s3 initialized. s1 becomes leader of term 1;\n  s1 receives 3 new commands{1, 2, 3}, sending to s2 and s3. And then these 3 logs are committed successfully;\n  s2 disconnects. it begins to increase the term number and request for votes. say it has reached term 3;\n  s1 crashes and s2 reconnects. Now s2 has a higher term and identical logs. s2 becomes the new leader of term 4;\n  s2 receives 1 new command{4}. Before it sends to s3, it disconnects;\n  s1 now reboots but r2 disconnects. s1 may wins the election again as it has the identical logs. Not s1 and s3 comes to term 5.\n  s1 receive 1 new command{5}. the command successfully got applied to s1 and s3 and got committed;\n  s1 crashes, s2 reconnects and s3 restarts. s2 may wins the election if it jumps to the term 5 and election timer times out before s1\u0026rsquo;s timer. Now s2 and s3 comes to term 5.\n  Since s3 restarts, it\u0026rsquo;s commitIndex gets reset to 0. So s2 will try to let s1 overwrite command{5} with command{4}. As now s1\u0026rsquo;s commitIndex is 0. It will commit the log at the same index again.\n  To solve this problem, when calling persist(), add rf.commitIndex as well. So the node is aware of how many logs have been committed after reboot. And it will reject AppendEntriesRPC request where args.PreLogIndex \u0026lt; rf.commitIndex, thinking it the same as the request with a smaller term number.\nUnreliable Network Besides, I sometimes have the failure where index out of range when trying to send AppendEntriesRPC and use rf.nextIndex[i] to slice the logs.\nTest (2C): Figure 8 (unreliable) ... panic: runtime error: slice bounds out of range [303:115] goroutine 17137 [running]: ~/6.824/src/raft.(*Raft).broadcastHeartbeat(0xc0001ce9a0, 0x59) ~/6.824/src/raft/raft_appendEntry.go:223 +0x617 created by ~/6.824/src/raft.(*Raft).setStateLeader ~/6.824/src/raft/raft.go:208 +0x156 exit status 2  And this error would only happen when having an unreliable network. When checking the log, I see it\u0026rsquo;s because rf.nextIndex[i] get increased multiple times while trying to send AppendEntriesRPC to follower. After review the code, I see:\nif reply.IsSuccess { rf.nextIndex[i] += len(args.Entries) }  This is the reason why this error showed up. The replies of multiple heartbeat request may comes together and all of them works. So the nextIndex got updated multiple times.\nThe easiest fix is using the values in the request directly to set instead of addition.\n// In case of unreliable network, running multiple times rf.nextIndex[i] = args.PrevLogIndex + len(args.Entries) + 1  References  Students' Guide to Raft ","date":"2021-01-07","permalink":"https://lianglouise.github.io/post/6.824_lab2c/","tags":["Distributed System"],"title":"Raft - 6.824 Lab2C"},{"content":"This post is about part A and B of Lab2 of MIT 6.824, Distributed Systems. It is about the implementation of Raft. Here in these two part, we only discuss the section up to the end of section 5 in the paper.\nBrief Idea In GFS or MapReduce, we have one single Master to manage the entire system. However, there is still a chance that the Master encounters some error or other issues. We than need a cluster to avoid this single point failure. And again, as in the distributed system, it\u0026rsquo;s inevitable to have some nodes failed. We need a way to keep the requests and results contestant across the cluster as recovery and always have one leader to take the requests from the users.\nRaft is a consensus algorithm for managing a replicated log. Here log refers to requests to be backed up in the cluster. Each node maintains a list of log of the same order. As long as morn than half of the nodes are alive in the network, the cluster can still be functional.\nThe system we are to build follows the rules illustrated as this chart in the paper:\nPart A: Election And State Transiting There are three different states in Raft: leader, follower and candidate. In a working Raft network, it is only allowed to have one leader to take the request and try to back up the request to the other nodes. And all other nodes become follower to receive log back up from leader and maintain a timer to start a leader election if not listening back from the leader after a certain time by converting itself to be an candidate. And Raft uses an monophonically increasing integer as term number to tell different leaders.\nFollowing the state chart above and RequestVoteRPC, we can see what we need to implement:\n​\t1) Initially, Set all the Raft struct to be follower. Each of them start a timer electionTimeOut of random duration. After timer time out, follower turns itself to be candidate, increase term by 1 and start a leader election;\n​\t2) candidate uses RequestVoteRPC to ask all the peers to vote; if winning the votes from the majority, turning itself to be new leader and start the go routine broadcastHeartbeat to tell all the peers they should reset the timer electionTimeOut and wait for the commands;\n​\t3) If unable to win the majority in the electionTimeOut, increase term by 1 and restart a leader election.\nThus, the Raft struct should be defined as:\nconst ( Follower = iota Leader Candidate ) type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer's state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer's persisted state me int // this peer's index into peers[] dead int32 // set by Kill() applyCh chan ApplyMsg state RafeState currTerm int votedFor int lastUpdate time.Time log []LogEntry commitIndex int lastApplied int // Leader use, reset for each term nextIndex []int // index of the next log entry to send to that server matchIndex []int // index of highest log entry known to be replicated on server // Candidate use voteCount int // Increase by 1 if revice a vote from a peer. }  A rule in the figure 2 that all the nodes must follow:\n​\tIf RPC request or response contains term T \u0026gt; currentTerm: set currentTerm = T, convert to follower (§5.1)\nSo that term number can keep up-to-date in the entire network and an out-dated leader can know it should jump to a higher term.\nElection Restrictions Besides, as the rules stated in the figure 2 of paper and section 5.4.1, we have several rules to implement as election restriction.\nFor candidate, it loses an election if one of the following happens:\n​\t1) It\u0026rsquo;s killed. rf.killed() == true;\n​\t2) Some RequestVoteRPCReply has a higher term number than it\u0026rsquo;s currTerm;\n​\t3) Receive AppendEntryRPC with a higher term number than it\u0026rsquo;s currTerm.\nFor follower, it rejects the RequestVoteRPC if one of the following happens:\n​\t1) the request from the candidate with a lower term;\n​\t2) the request from the candidate which comes later and belongs to the same term;\n​\t3) the last entry having a higher term;\n​\t4) the logs longer than candidate when having the same term number.\nVoteFor The voteFor should be carefully managed, as failing to follow the rule in the figure 2 would lead to unexpected election result, especially when the nodes need to use records to persisted record to reboot in the part C.\nIf the node is a candidate, set its voteFor = rf.me\nIf the node receives a RequestVoteRPC  and replies voteGranted == true, then it should set its voteFor = candidateId\nIf the node becomes a followers only because receiving a RPC request from a node in a higher term and update itself to a follower , then it should set its voteFor = -1\nPart B: Log Replication In this part, Raft struct will receive logs from the test by rf.Start() to reach agreement among the peers and the log replication is one of the most important part of Raft.\nThe logic is quite straightforward:\n​\t1) leader receive a new command from the user through rf.Start() and appends this new log to the end of the rf.log[];\n​\t2) leader can call peers' AppendEntryRPC immediately after receiving one new log but also wait until leader calls broadcastHeartbeat to send all the new logs together, which is a good way to avoid calling AppendEntryRPC too frequently and have concurrent update issues with rf.nextIndex[] and rf.matchIndex;\n​\t3) follower receives a AppendEntryRPCArgs, comparing rf.log[args.PreLogIndex].Term with args.PreLogTerm. If equaling, follower can reach a agreement up to args.PreLogIndex then append the new entries in the request:\n​\tappend(rf.log[:args.PreLogIndex+1], args.Entries...)\n​\tOtherwise, follower replies with false, asking leader to decrease the args.PreLogIndex recursively until finding some index where leader and follower can reach an agreement. follower should abort the logs between args.PreLogIndex and rf.lastApplied;\n​\t4) After appending new log entries successfully, leader updates index value of rf.matchIndex[peerIdx] and then update its commitIndex by choosing a N where a majority of rf.matchIndex[i] \u0026gt;= N and N \u0026gt; rf.commitIndex;\n​\t5) In the next broadcastHeartbeat, follower receives the leader\u0026rsquo;s new commitIndex in args.LeaderCommit. If args.LeaderCommit \u0026gt; rf.commitIndex, then follower should update its commitIndex to be:\n​\tmin(args.LeaderCommit, rf.lastApplied)\n​\t6) Both leader and follower need to send logs between old and new commit Index to the channel ApplyMsg for testing.\nWe also need to be careful about the logic of handler AppendEntries:\nfunc (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.mu.Lock() defer rf.mu.Unlock() // Update state as term number in the args if args.Term \u0026lt; rf.currTerm || (args.Term == rf.currTerm \u0026amp;\u0026amp; rf.state == Leader) { // If receving a log with outdated term number, rejecting directly // If appearing 2 leaders, rejecting reply.IsSuccess = false reply.Term = rf.currTerm return } else if args.Term \u0026gt; rf.currTerm { // See a higher term turn to follower of term rf.setStateFollower(args.Term, -1) } else { // If same term just refresh the update time rf.lastUpdate = time.Now() } // Only with a valid term number, it should be considered as hearing from the leader // Update rf.log[] if args.PreLogIndex \u0026gt; rf.lastApplied { // The expected PreLogIndex \u0026gt; actual last index on follower. Ask for decrement reply.IsSuccess = false } else if rf.log[args.PreLogIndex].Term == args.PreLogTerm { // Able to reach agreement at the PreLogIndex, concat rf.log reply.IsSuccess = true //... } else if rf.log[args.PreLogIndex].Term != args.PreLogTerm { // Unable to reach agreement at the PreLogIndex due to differetn term number. Ask for decrement reply.IsSuccess = false } // If success on agrement and args.LeaderCommit \u0026gt; rf.commitIndex, // update follower's commit index and send apply msg to tester return }  Tips   Use go routine to send RPC parallelly. Also use go routine to send ApplyMsg as after commitment, they are safe to leave them.\n  Split the raft.go into multiple go files by the RPC types, as for each RPC you may need one handler for follower and multiple functions for leader;\n  For node state transiting, a good idea is to use set methods to group all the variables you need to update for state change so you won\u0026rsquo;t miss some of them.\nUse changing to Follower as an example:\nfunc (rf *Raft) setStateFollower(term, voteFor int) { rf.currTerm = term rf.votedFor = voteFor // Update last time hear from the leader rf.state = Follower rf.lastUpdate = time.Now() // Start a new election timer go rf.electionTimeOut(term) }    Use sort.Slice() to help leader find new commitIndex;\n  Election and heartbeat timers are suggested to implement using for {} instead of time.NewTimer. I also pass in a term to the timer so that when using set methods to update node\u0026rsquo;s state, you don\u0026rsquo;t have to warry about if you need to restart a new timer or not.\nUse heartbeat as an example:\nconst ( HeartbeatInterval = 120 * time.Millisecond ) func (rf *Raft) broadcastHeartbeat(term int) { for !rf.killed() { rf.mu.Lock() // Stop sent heartbeats if not a leader or has jumped to another term if rf.state != Leader || rf.currTerm \u0026gt; term { rf.mu.Unlock() break } // Some other operations in between... // Send heartbeat to all other peers for i, _ := range rf.peers { if i == rf.me { continue } // You can construct AppendEntriesArgs here or in the function go rf.sendAppendEntries(i) } rf.mu.Unlock() time.Sleep(HeartbeatInterval) } }    Always check Raft struct\u0026rsquo;s state and return value of rf.killed() after acquiring the lock. Return the function of break the loop if state changed or being killed.\n  References  Raft Visualization Students' Guide to Raft ","date":"2021-01-02","permalink":"https://lianglouise.github.io/post/6.824_lab2ab/","tags":["Distributed System"],"title":"Raft - 6.824 Lab2A 2B"},{"content":"This is the first lab of MIT 6.824, Distributed Systems. It is about the implementation of MapReduce, a framework introduced by Google that can process huge volume of data parallelly by splitting a single task to multiple small ones and assigning them to a cluster of \u0026ldquo;cheap\u0026rdquo; machines. And by using this framework, also as mentioned in the paper, this lets programmer make use of distributed system without the need of experience of it.\nA very classic use case of this framework is counting words in a large file, which is also what we are to implement.\nConcept Behind the Scene The overview of this framework is illustrated as the Figure 1 in the paper. There will be one Master and many Worker.\nMaster will assign tasks to Worker to execute and monitor the progress. It receives m input files and will generate r output files.\nWorker mainly works on two things:\n During Map phase, each Worker reads one of m input files, apply it to user-defined Map() which returns some \u0026lt;key, value\u0026gt; pairs. Then save them into intermediate files. Usually, there will be m Map tasks in total. During Reudece phase, each Worker reads the \u0026lt;key, value\u0026gt; pairs in the corresponding intermediate files and apply them to Reduce() and save the result to output file. There will be r Reduce tasks in total.  Implementation in the Lab We are required to implement four major components in the framework: Task, RPC, Workerand Master .\nTask This is a Go struct sent back-and-forth between Master and Worker.\nMaster -\u0026gt; Worker: start a new task.\nWorker -\u0026gt; Master: report the task result.\ntype MapReduceTask struct { // 0: Map, 1: Reduce, 2: Exit, 3: Wait TaskType int // 0: Unassigned, 1: Assigned, 2: Finished Status int // Start Time TimeStamp time.Time // Index in the list of tasks Index int InputFiles []string OutputFile []string }  For Map task, it\u0026rsquo;s expected to have 1 file name in InputFiles and nReduce of file name in OutputFile.\nSimilarly, for Reduce task, it\u0026rsquo;s expected to have nReduce of file name in InputFiles and 1 file name in OutputFile.\nRPC Since Master and Worker are different processes, we are asked to use Remote procedure call to send request and response between Master and Worker.\nIn this lab we can create two RPC, one for requesting a new task and one for submitting a task:\ntype RequestTaskArgs struct {} type RequestTaskReply struct { NReduce int Task MapReduceTask } type SubmitTaskArgs struct { Task MapReduceTask } // Here Master is always available type SubmitTaskReply struct {}  Worker worker is kind of single thread. It keeps requesting new task, processing it, report it and exit when master sends signal to exit.\nfunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { for { args := RequestTaskArgs{} reply := RequestTaskReply{} res := call(\u0026quot;Master.RequestTask\u0026quot;, \u0026amp;args, \u0026amp;reply) if !res { break } switch reply.Task.TaskType { case MAP: doMap(\u0026amp;reply, mapf) case REDUCE: doReduce(\u0026amp;reply, reducef) case WAIT: time.Sleep(1 * time.Second) case EXIT: os.Exit(0) } } }  Master The responsibilities for master are:\n  assigning the tasks to different worker. And if some worker does not report task back after certain time (10 sec here), reassign the task to another worker;\n  monitoring the progress. If all the map tasks are done, worker should start to assign reduce tasks. When all the reduce tasks are done, master needs to tell the worker to exit;\n  validating the result. Only confirm the worker\u0026rsquo;s output files are valid when the task is completed and submitted.\n  So the Master should be like below\ntype Master struct { inputFiles []string nReduce int mapTasks []MapReduceTask reduceTasks []MapReduceTask // Increase by 1 when one mapTask done. The map Phase is done when mapDone == inputFiles mapDone int // Increase by 1 when one reduceTask done. The reduce Phase is done when reduceDone == nReduce reduceDone int // Each time allow one work to update mutex sync.Mutex }  and when initializing it:\nfunc MakeMaster(files []string, nReduce int) *Master { m := Master{ inputFiles: files, nReduce: nReduce, mapTasks: make([]mapReduceTask, len(files)), reduceTasks: make([]mapReduceTask, nReduce), mapDone: 0, reduceDone: 0, mutex: sync.Mutex{}, } // Fill each task in array mapTasks with the input file name and itialize other fileds // Collect arrray for InputFiles and OutputFile in two tasks array // ...... return \u0026amp;m }  Tips  worker sometimes fails when processing the task. It might happen to write result to output files halfway. To avoid those garbage output, worker should be designed to write to an temp file and only when the entire task gets submitted, master then marks them are valid output.  // Worker create a new temp output file tmpFile, err := ioutil.TempFile(\u0026quot;./\u0026quot;, \u0026quot;mr\u0026quot;) // Master rename them when recv a submit request for _, file := range reply.Task.OutputFile { // validName := // mapTask: mr-\u0026lt;mapTask_idx\u0026gt;-\u0026lt;reduceTask_idx\u0026gt; //\treduceTask: mr-out-\u0026lt;reduceTask_idx\u0026gt; err := os.Rename(tempOut, validName) if err != nil { panic(err) } }   During the map phase, worker should use ihash(key) % NReduce as the reduce task number to group the result \u0026lt;key, value\u0026gt; pair into the same array.\n  All the field names in the RPC Args and Reply should begin with capital letter or Go will not send them.\n ","date":"2020-12-23","permalink":"https://lianglouise.github.io/post/6.824_lab1_mapreduce/","tags":["Distributed System"],"title":"MapReduce - 6.824 Lab1"},{"content":"After doing a handshake using X3DH, Both users can authenticate each other and agree on a shared master secret.\nSo what\u0026rsquo;s next?\nThe easiest solution is do as TLS: client and server share client_write_{key, iv} and server_write_{key, iv} and use a per-record sequence number to encrypt and decrypt messages in the session. However, as said in the previous post, in the situation of instant messaging software, one side of client may be offline for a long time. As a result , there exist some risk that once the client_write_{key, iv} and server_write_{key, iv} are compromised (though it\u0026rsquo;s kind of hard by brute forcing), all the future messages will be transparent to Mallory.\nThus, it\u0026rsquo;s very important to introduce an algorithm that add extra entropy while don\u0026rsquo;t have handshake again to generate a new master secret (which is quite expensive). This is why Double Ratchet was designed.\nHere I am using Double Ratchet without header encrypted as example.\nThe Key Chain The most important thing in Double Ratchet is maintaining key chains with three properties:\n  Resilience: The output keys appear random to an adversary without knowledge of the KDF keys. This is true even if the adversary can control the KDF inputs.\n  Forward security: Output keys from the past appear random to an adversary who learns the KDF key at some point in time.\n  Break-in recovery: Future output keys appear random to an adversary who learns the KDF key at some point in time, provided that future inputs have added sufficient entropy.\n  And each user will need to maintain three key chains similar to TLS except the root key chain:\n  Root chain: generate new root key for new write/read key and iv once receiving some new messages;\n  Writing chain: generate new write key and iv so user can encrypt messages with different key and iv while no new responses back;\n  Reading chain: generate new read key and iv to corresponding to the write keys and ivs in sequences for decryption.\n  Here writing chain and reading chain are very similar. They are both symmetric chains as their value only depends on the output value from root chain and previous write/read key.\nThe reason to introduce root chain is that writing chain and reading chain are both symmetric, once Mallory managed to steal some KDF output, this is no forward secrecy. However, The root chain\u0026rsquo;s key value depends on the previous root key and a new key generated by DH exchange (e.g. X22519). This will ensure that there won\u0026rsquo;t be a too long writing chain or reading chain.\nAs shown in the picture above, the new root key and new write/read key will be generated once Bob send a new X25519 Public key in the header of a new message.\nState Storing It\u0026rsquo;s very important to keep track of chains so the program can tell use which key to encrypt or decrypt:\nFor each user we need keep a record like this:\n{ \u0026quot;bob\u0026quot;: { \u0026quot;RK\u0026quot;: \u0026quot;The latest rook key input material\u0026quot;, \u0026quot;DH_pair\u0026quot;: [\u0026quot;Alice's current private key\u0026quot;, \u0026quot;Alice's current public key\u0026quot;], \u0026quot;DH_p\u0026quot;: \u0026quot;Current DH remote public key(Bob)\u0026quot;, \u0026quot;CKs\u0026quot;: [\u0026quot;list of the latest write chain key in each round\u0026quot;], \u0026quot;CKr\u0026quot;: [\u0026quot;list of the latest read chain key in each round\u0026quot;], \u0026quot;PN\u0026quot;: \u0026quot;# of DH process to generate new root key\u0026quot; } }  It\u0026rsquo;s update to you decide, the max number of write key and read key to keep in the list\nNote: we just need save the latest root key as for previous rounds write key or read key has been generated. So does DH_pair.\nThen in code:\n# Continue in Class Client from previous def dr_state_initialize(self, user_name, RK, DH_pair, DH_p): self.dr_keys[user_name] = { \u0026quot;RK\u0026quot;: RK, \u0026quot;DH_pair\u0026quot;: DH_pair, \u0026quot;DH_p\u0026quot;: DH_p, \u0026quot;CKs\u0026quot;: [], \u0026quot;CKr\u0026quot;: [], \u0026quot;Ns\u0026quot;: 0, \u0026quot;Nr\u0026quot;: 0, \u0026quot;PN\u0026quot;: 0 }  Integration with X3DH Double Ratchet is usually integrated with X3DH. We can initialize user state with the result from X3DH.\n  Alice (X3DH Sender):\n DH_pair: The EK, ephemeral key pair; DH_p: remain blank, waiting for Bob\u0026rsquo;s first response in DR format.    Bob (X3DH Receiver):\n DH_pair: remain blank, generate when send first message in DR format; DH_p: The EK_pa from X3DH Hello message.    Core Functions Following are the core functions implemented following the recommendations from Signal.\nKey Generation functions The KDF in the chains create new keys and iv to move the ratchet.\n  Root Key KDF:\nCreate rk_input_material for next round root key ratchet and ck for this round.\nBoth 32 bytes long\n  from Crypto.Protocol.KDF import HKDF def KDF_RK(rk, dh_out): out = HKDF(dh_out, 64, rk, SHA256, 1) rk_input_material = out[:32] ck = out[32:] return rk_input_material, ck    Write/Read Chain Key KDF:\nCreate ck_input_material for next round and mk to encrypt/decrypt this round\u0026rsquo;s message.\nBoth are 32 bytes long.\nck_input_material: use constant b'\\x01' as message input;\nmk: use constant b'\\x02' as message input.\n  from Crypto.Protocol.KDF import HKDF def KDF_CK(ck): ck_input_material = HMAC.new(ck, digestmod=SHA256).update(b'\\x01').digest() mk = HMAC.new(ck, digestmod=SHA256).update(b'\\x02').digest() return ck_input_material, mk  Data Encryption To generate the ciphertext in ENCRYPT() we need use mk to create keys K by:\nK = HKDF(mk, 80, b'\\0' * 80, SHA256, 1)  then sign header||plaintext||dh_pub_b using HMAC.new(K[32:64], digestmod=SHA256)\nthen pad signature||plaintext with empty bytes to make it fit AES Block Size,\nthen encrypt padded signature||plaintext using AES.new(K[:32], AES.MODE_CBC, iv=K[64:])\nFinally concat ciphertext as:\niv||AES(K[:32], Sig(K[32:64], header||plaintext||dh_pub_b), plaintext).\nPayload Format We can use JSON to separate header and ciphertext:\n{ \u0026quot;header\u0026quot;: { \u0026quot;dh_p\u0026quot;: \u0026quot;\u0026lt;New DH public key\u0026gt;\u0026quot;, \u0026quot;pn\u0026quot;: \u0026quot;\u0026lt;# of DH process to generate new root key(Including generating this message)\u0026gt;\u0026quot;, \u0026quot;n\u0026quot;: \u0026quot;\u0026lt;# of Message sent (Including this)\u0026gt;\u0026quot; }, \u0026quot;ciphertext\u0026quot;: \u0026quot;\u0026lt;output of ENCRYPT(mk, plaintext, header)\u0026gt;\u0026quot; }  Ratchet Update For each user, the key chains updated in the conditions below:\n  Sending:\n  Generate a new root key as send chain root key:\n At the initial state, there is no DH_pair to use; After receiving a new DH_p in the message header.    Use the existing send chain key:\n Try to send a message when not receive a new DH_p yet.      Receiving:\n  Generate a new root key as read chain root key:\n When there is a different DH_p in message header and having a bigger PN    Use existing read chain key:\n When message header has a smaller PN than the local one      Fin Though Double Ratchet is quite complicated. But keep in mind that, this is like playing a ping-pong game, update the root key and stop using the latest write key chain only when the \u0026lsquo;ball\u0026rsquo; is back.\nCertainly, the double ratchet and X3DH can be more secure, compared to TLS. However, this still only protects the application layer. But it\u0026rsquo;s still a very good solution since almost every key is dynamic in a long term session.\n","date":"2020-11-10","permalink":"https://lianglouise.github.io/post/some_practice_on_implementing_signal_protocol_with_python_2/","tags":["web","security"],"title":"Some Practice on Implementing Signal Protocol With Python (2): Double Ratchet"},{"content":"Lately I was working on a Web Security Project and I came across this very interesting Web Messaging Encryption Protocol - Signal Protocol.\nWhen doing web communication encryption, the most common solution is using TLS, where basically, two sides handshake using Diffie–Hellman key exchange through an insecure channel and communicate using symmetric encryption (i.e.: AES) based on the secret key SK derivated from DH key exchange. This solution can provide strong enough encryption when communicating between client and server.\nHowever, when it comes to the situation where client talks to client (E2E) and server just redirects message, especially instant messaging, there are two major security issues we need to solve:\n  The server itself may be not secure and we are unable to trust it to store the messages in clear or weak encryption;\n  If always relying on the same SK, this gives Mallory time to crack it out and every message would be able to be decrypted.\n  To achieve secure E2E communications, the signal protocol was thus introduced. It has two main fragments:\n  Extended Triple Diffie-Hellman(X3DH): This extended DH key exchange helps two clients establish a shared SK for future communications in an asynchronous way, only requiring two clients publish some keys to server;\n  Double Ratchet algorithm: This algorithm provides both forward secrecy and break-in recovery, which makes each message have different SK so there is no way to crack one SK and know everything.\n  The specifications from Signal Protocol provide a relatively high level overview but there are still some details not that clear and hard to understand. So I would like to talk about how I understand this protocol with some actual implementation in Python. In this post, I will focus on the X3DH part.\nX3DH Protocol How shared DH Value computed in X3DH is using X25519 Key exchange which is a Elliptic-curve Diffie–Hellman. And I chose library cryptography, which is part of pyOpenSSL, to compute this X25519 Key exchange.\nThe basic workflow of establishing a session using X3DH is illustrated as below:\nPreparation At first, both clients need to generate required key pairs, publish key bundles to server and save the secrets so they are able to start the session.\nKey pairs to generate:\n  IK: Long-Term Identity Key (32 bytes both), which is an unique identifier for each client;\n  SPK: Signed PreKey (32 bytes both), a key pair will be revoked and re-generated every few days/weeks for sake of security. Alongside, SPK_sig: SPK public key\u0026rsquo;s signature, signed by IK secret key - SIG(IK_s, SPK_p);\n  OPK: One-time Off Key (32 bytes both), a key pair will be revoked once used for handshake. Usually, the client will generate multiple OPK pair and generate new one once server used up or needs more.\n  Then all these key pair\u0026rsquo;s public keys and SPK_sig will be sent to server.\nfrom cryptography.hazmat.primitives.asymmetric import x25519 from XEdDSA import sign class User(): def __init__(self, name, MAX_OPK_NUM): self.name = name self.IK_s = x25519.X25519PrivateKey.generate() self.IK_p = self.IK_s.public_key() self.SPK_s = x25519.X25519PrivateKey.generate() self.SPK_p = self.IK_s.public_key() self.SPK_sig = sign(IK_s, SPK_p) self.OKPs = [] self.OPKs_p = [] for i in range(MAX_OPK_NUM): sk = x25519.X25519PrivateKey.generate() pk = sk.public_key() self.OPKs_p.append(pk) self.OKPs.append((sk, pk)) # for later steps self.key_bundles = {} self.dr_keys= {} def publish(self): return { 'IK_p': self.IK_p, 'SPK_p': self.SPK_p, 'SPK_sig': self.SPK_sig, 'OPKs_p': self.OPKs_p }  Due to unable to find out how to do XEdDSA signature required by signal protocol while using cryptography, since each library has their different way to format their X25519 Keys, here I picked a mock function sign. You could refer to this blog post and RFC Standard to convert between Ed25519 and X25519\nEstablish the Session To actually establish the session, steps 3-5 in the diagram above will be carried out by Alice and Bob.\n First, Alice tries to send first message. Her client will ask server for Bob\u0026rsquo;s key bundle and generate a Ephemeral Key pair use only for this handshake:  # Continue in Class Client # Get key bundle from a server object def get_key_bundle(self, server, user_name): if user_name in self.key_bundles and user_name in self.dr_keys: print('Already stored ' + user_name + ' locally, no need handshake again') return False self.key_bundles[user_name] = server.get_key_bundle(user_name) return True def initial_handshake(self, server, user_name): if get_key_bundle(user_name) # Generate Ephemeral Key sk = x25519.X25519PrivateKey.generate() self.key_bundles[user_name]['EK_s'] = sk self.key_bundles[user_name]['EK_p'] = sk.public_key() return    Then, Alice\u0026rsquo;s client will compute Alice\u0026rsquo;s secret key SK, with\n  IK_sa Alice\u0026rsquo;s secret Identity key,\n  EK_pk Alice\u0026rsquo;s public Ephemeral Key,\n  IK_pb Bob\u0026rsquo;s public Identity Key,\n  SPK_pb Bob\u0026rsquo;s public Signed PreKey,\n  OPK_pb Bob\u0026rsquo;s public One-time Off key\n  getting 4 DH values and derivate a 32 bytes SK, where SK = HKDF(DH_1||DH_2||DH_3||DH_4)\nfrom Crypto.Protocol.KDF import HKDF from Crypto.Hash import SHA256 from XEdDSA import verify KDF_F = b'\\xff' * 32 KDF_LEN = 32 KDF_SALT = b'\\0' * KDF_LEN # Continue in Class Client def x3dh_KDF(key_material): km = KDF_F + key_material return HKDF(km, KDF_LEN, KDF_SALT, SHA256, 1) def generate_send_secret_key(self, user_name): key_bundle = self.key_bundles[user_name] DH_1 = self.IK_s.exchange(key_bundle['SPK_p']) DH_2 = key_bundle['EK_s'].exchange(key_bundle['IK_p']) DH_3 = key_bundle['EK_s'].exchange(key_bundle['SPK_p']) DH_4 = key_bundle['EK_s'].exchange(key_bundle['OPK_p']) if not verify(self.IK_s, key_bundle['SPK_sig']): print('Unable to verify Signed Prekey') return # create SK key_bundle['SK'] = x3dh_KDF(DH_1 + DH_2 + DH_3 + DH_4)  The HKDF here I pick is from pycryptodome. And note that X3DH requires HKDF function prepend 32 b'\\xff' bytes if curve is X25519, and 57 b'\\xff' bytes if curve is X448. The salt should a b'\\0' byte sequence of length equals to length of output key length. Hash functions are required to be a 256-bit or 512-bit function.\n  Then, Alice\u0026rsquo;s client will build the hello message to send out:\nThe format of the initial message:\nIK_pb||EK_pa||OPK_pb||n_0||t_0||AES(SK, SIG(IK_sa, IK_pa||EK_pa||OPK_pb||AD)||IK_pa||IK_pb||AD)\nand you could set the AD\u0026rsquo;s format like this:\n{ \u0026quot;from\u0026quot;: \u0026quot;alice\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;bob\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;\u0026lt;some greeting messages\u0026gt;\u0026quot; }  Implement in Python:\nfrom cryptography.hazmat.primitives import serialization from Crypto.Random import get_random_bytes from Crypto.Cipher import AES import json # Length definition for hello message encryption AES_N_LEN = 16 AES_TAG_LEN =16 # Continue in Class Client def dump_privatekey(private_key, to_str=True): private_key = private_key.private_bytes( encoding=serialization.Encoding.Raw, format=serialization.PrivateFormat.Raw, encryption_algorithm=serialization.NoEncryption() ) return private_key def dump_publickey(public_key): public_key = public_key.public_bytes( encoding=serialization.Encoding.Raw, format=serialization.PublicFormat.Raw ) return public_key def build_x3dh_hello(self, server, to, ad): # Binary additional data b_ad = (json.dumps({ 'from': self.name, 'to': to, 'message': ad })).encode('utf-8') key_bundle = self.key_bundles[to] # 64 byte signature key_comb = dump_publickey(self.IK_p) + dump_publickey(key_bundle['EK_p']) +dump_publickey(key_bundle['OPK_p']) signature = sign(self.IK_s, key_comb + b_ad) print(\u0026quot;Alice message signature: \u0026quot;, signature) print(\u0026quot;data: \u0026quot;, key_comb + b_ad) # 16 byte aes nonce nonce = get_random_bytes(AES_N_LEN) cipher = AES.new(key_bundle['SK'], AES.MODE_GCM, nonce=nonce, mac_len=AES_TAG_LEN) # 32 + 32 + len(ad) byte cipher text ciphertext, tag = cipher.encrypt_and_digest(signature + dump_publickey(self.IK_p) + dump_publickey(key_bundle['IK_p']) + b_ad) # initial message: (32 + 32 +32) + 16 + 16 + 64 + 32 + 32 + len(ad) message = key_comb + nonce + tag + ciphertext server.send(to, message) # For Double Ratchet self.initialize_dr_state(to, key_bundle['SK'], [key_bundle['EK_s'], key_bundle['EK_p']], \u0026quot;\u0026quot;)  AES refers AES256 in GCM Mode, n_0 and t_0 are nonce and tag, using SK as key, which is also from pycryptodome.\n  Finally, Bob receives first message, decrypts and verifies it:\nBob will also check out Alice\u0026rsquo;s key bundle from server and manipulate the hello message to compute his SK. And then decrypt the message and verify the signature in the plaintext for AEAD.\nThe verifications include:\n  verify public signed PreKey\u0026rsquo;s signature SPK_sig;\n  verify IK_pb and OPK_pb in the hello message and in the local db matches;\n  verify IK_pa in the hello message and in key bundles matches;\n  verify AD, the json object in the hello message has correct from and to.\n  EC_KEY_LEN = 32 # Continue in Class Client def recv_x3dh_hello_message(self, server): # receive the hello message sender, recv = server.get_message() self.get_key_bundle(server, sender) key_bundle = self.key_bundles[sender] IK_pa = recv[:EC_KEY_LEN] EK_pa = recv[EC_KEY_LEN:EC_KEY_LEN*2] OPK_pb = recv[EC_KEY_LEN*2:EC_KEY_LEN*3] nonce = recv[EC_KEY_LEN*3:EC_KEY_LEN*3+AES_N_LEN] tag = recv[EC_KEY_LEN*3+AES_N_LEN:EC_KEY_LEN*3+AES_N_LEN+AES_TAG_LEN] ciphertext = recv[EC_KEY_LEN*3+AES_N_LEN+AES_TAG_LEN:] # Verify if the key in hello message matches the key bundles from server if (IK_pa != key_bundle['IK_p']): print(\u0026quot;Key in hello message doesn't match key from server\u0026quot;) return # Verify Signed pre key from server if not verify(key_bundle['IK_p'], key_bundle['SPK_sig']): print('Unable to verify Signed Prekey') return sk = create_recv_secret_key(IK_pa, EK_pa, OPK_pb) print('bob sk: ', sk) if sk is None: return key_bundle['SK'] = sk message = x3dh_decrypt_and_verify(self, key_bundle, IK_pa, EK_pa, nonce, tag, ciphertext) # For Double Ratchet self.initialize_dr_state(sender, sk, [], EK_pa) # Get Ek_pa and plaintext ad return EK_pa, message def generate_recv_secret_key(self, IK_pa, EK_pa, OPK_pb)): # Find corresponding secret OPK secret key # And remove the pair from the list OPK_sb = self.search_OPK_lst(OPK_pb) if OPK_sb is None: return IK_pa = x25519.X25519PublicKey.from_public_bytes(IK_pa) EK_pa = x25519.X25519PublicKey.from_public_bytes(EK_pa) DH_1 = self.SPK_s.exchange(IK_pa) DH_2 = self.IK_s.exchange(EK_pa) DH_3 = self.SPK_s.exchange(EK_pa) DH_4 = OPK_sb.exchange(EK_pa) # create SK return x3dh_KDF(DH_1 + DH_2 + DH_3 +DH_4) def x3dh_decrypt_and_verify(self, key_bundle, IK_pa, EK_pa, nonce, tag, ciphertext): # Decrypt cipher text and verify cipher = AES.new(decodeB64Str(sk), AES.MODE_GCM, nonce=nonce, mac_len=AES_TAG_LEN) try: p_all = cipher.decrypt_and_verify(ciphertext, tag) except ValueError: print('Unable to verify/decrypt ciphertext') return except Exception as e: print('e') return # Byte format of plain text sign = p_all[:EC_SIGN_LEN] IK_pa_p = p_all[EC_SIGN_LEN:EC_SIGN_LEN+EC_KEY_LEN] IK_pb_p = p_all[EC_SIGN_LEN+EC_KEY_LEN:EC_SIGN_LEN+EC_KEY_LEN*2] ad = p_all[EC_SIGN_LEN+EC_KEY_LEN*2:] if (IK_pa != IK_pa_p and IK_pb != IK_pb_p): print(\u0026quot;Keys from header and ciphertext not match\u0026quot;) return if not verify(IK_pa], sign, IK_pa_p + EK_pa + OPK_pb + ad): print(\u0026quot;Unable to verify the message signature\u0026quot;) return print('Message: ', json.loads(ad)) return json.loads(ad)    Next Step After both Alice and Bob share a SK, X3DH can be marked as completed. Though this process is hard to be cracked out, we have to integrate it with Double Ratchet Algorithm to reach ultimate secure in application layer (Again, this only encrypts the request/response data but not IP/TCP or others). And I will talk about it in the next part.\nFor now, we just need to make sure what to keep for DR integration and what to destroy to avoid reuse attack.\nAlice, Bob and server need to destroy the OPK from Bob used in this handshake.\nAlice needs to keep EK pair and SK to initiate DR.\nBob also needs to keep Alice\u0026rsquo;s EK_p and SK to initiate DR and send his response using DR.\n","date":"2020-10-20","permalink":"https://lianglouise.github.io/post/some_practice_on_implementing_signal_protocol_with_python_1/","tags":["web","security"],"title":"Some Practice on Implementing Signal Protocol With Python (1): X3DH"},{"content":"Welcome to my github page.\nFinally! At the end of 2019, I managed to launched my personal blog with the help of Hexo and archer theme. with the help of Hugo and Fuji theme.\nI will share my naive thoughts and experiences on coding here. 😜\nHope you could find something helpful and interesting.\n","date":"2019-12-31","permalink":"https://lianglouise.github.io/post/hello_world/","tags":["Intro"],"title":"Hello-World!"}]