[{"content":"QPACK is a header field compression format for HTTP/3 that makes HTTP/2â€™s HPACK header compression format compatible with the QUIC protocol.\nIn HTTP/3, the way the sender\u0026rsquo;s encoder and the receiver\u0026rsquo;s decoder reach agreement on the the state of the dynamic table for compression would be very different compared to HTTP/2. In this post, I would like to talk some of understanding I gained while I was reading the RFC and implementing QPACK.\n mermaid.initialize({startOnLoad:true, theme: \"neutral\" }); Why Different? HPACK doesn\u0026rsquo;t need the endpoint to communicate directly with its peer for the most of time (unless there is a change in the dynamic table size). Since all HTTP/2 frames in different stream will always be transmitted in order when they are transmitted over TCP. So we don\u0026rsquo;t need to worry if the decoder\u0026rsquo;s dynamic table will somehow get into a state different from the encoder\u0026rsquo;s at all.\nNevertheless, due to the nature of QUIC, which only guarantees that data in the same stream would \u0026ldquo;arrive in order\u0026rdquo; at the application layer, QPACK requires the sender\u0026rsquo;s encoder to start a HTTP/3 unidirectional stream Encoder Stream and the receiver\u0026rsquo;s decoder to start a HTTP/3 unidirectional stream Decoder Stream to sync the states of the dynamic table at both sides.\nCertain encoder instructions and decoder instructions are consequently defined in QPACK\u0026rsquo;s RFC to modify the peer\u0026rsquo;s state.\nDynamic Table How dynamic tables are maintained on its own has no big differences compared to HPACK.\nIt\u0026rsquo;s still a FIFO list with a max capacity. Any entries referenced in the HEADERS frames will be marked and can not be evicted whereas the unreferenced entries thus can be evicted if space is needed when inserting new entries.\nThe initial capacity of the dynamic table is going to be zero at both sides. If the receiver\u0026rsquo;s decoder broadcasts its setting SETTINGS_QPACK_MAX_TABLE_CAPACITY with a non-zero value and the sender\u0026rsquo;s encoder has its maximum capacity max_cap a non-zero value as well, then the encoder can determine the dynamic table capacity to be used in this pair no more than min(SETTINGS_QPACK_MAX_TABLE_CAPACITY, max_cap).\nInstructions Instructions are defined to help sync the states of the dynamic table at both sides. Note that if dynamic table is not going to be in the connection, then these instruction are not necessarily being used. All these instructions will be sent unframed through QUIC\u0026rsquo;s unidirectional stream.\nEncoder Instructions Encoder can emit four kinds of instructions of two categories:\n  Instruction Set Dynamic Table Capacity. This will make the decoder to change its dynamic table capacity. A typical use case would be, the sender sets its actual dynamic capacity after receiving the SETTINGS frames. It will rely on this instruction to let the decoder change its initial capacity to the same as the encoder\u0026rsquo;s (RFC9204 section.3.2.2).\n  Instruction Insert with Name Reference, Insert with Literal Name and Duplicate. After the sender inserts a new entry to its dynamic table. One of these instructions would be emitted, the decoder will insert the same entry to its dynamic table by decoding the instruction. The exact type of instruction would be used depends on the state before the new entry was inserted into the encoder\u0026rsquo;s dynamic table. If name:value can be found, then a Duplicate would be used. If only name can be found, Insert with Name Reference will be used. Otherwise, the instruction will be Insert with Literal Name.\n  Decoder Instructions The instructions sent by the decoder would be mainly for the decoder to notify the encoder on different events in HTTP/3 and QPACK:\n  Instruction Section Acknowledgment. It\u0026rsquo;s would be emitted once the decoder successfully decodes the field sections in HEADERS frames in the stream which are referred to the encoder\u0026rsquo;s dynamic table entries. Then the encoder can clear the references to the corresponding entries in its dynamic table.\n  Instruction Insert Count Increment. This instruction is quite similar to the previous one, used for acknowledgement that new entries have been received by the decoder and properly inserted into its dynamic table. Once the encoder received this instruction, then it can know that what entries it can safely use for encoding without potentially blocking the request/push stream.\n  Instruction Stream Cancellation. Once the receiver decides to reset the stream for the reasons other before properly close the stream (i.e., a client resets a stream because the frames sequence on that stream is wrong), the endpoint needs to emit this instruction to notify the sender\u0026rsquo;s encoder that all references associated with this stream should be removed.\n  A Typical Session To give a better and clearer idea how QPACK works between two endpoints, below is a sample sequence diagram. Since there are going to be two pairs of QPACK\u0026rsquo;s encoder and decoder between client and server\u0026rsquo;s HTTP/3 connection, here we focus on the client\u0026rsquo;s encoder and the server\u0026rsquo;s decoder, as the other pair will behave the same.\nsequenceDiagram box client participant Client's encoder participant Client end box server participant Server participant Server's decoder end Client-\u0026gt;\u0026gt;Server: Control Stream, Client's SETTINGS frame rect rgb(93, 173, 226) Server-\u0026gt;\u0026gt;Client: Control Stream, Server's SETTINGS frame \u0026lt;br/\u0026gt;(contains Non-zero SETTINGS_QPACK_MAX_TABLE_CAPACITY) Client's encoder-\u0026gt;\u0026gt;Server's decoder: Starts Encoder Stream, and \u0026lt;br/\u0026gt; sends Set Dynamic Table Capacity instruction to enable dynamic table Server's decoder-\u0026gt;\u0026gt;Client's encoder: Starts Decoder stream. (Server can also wait until \u0026lt;br/\u0026gt; there are entries inserted into the dynamic table then start this) end Server--\u0026gt;Client: The same actions happens between \u0026lt;br/\u0026gt; the client's decoder and server's encoder rect rgb(93, 173, 226) Client--\u0026gt;\u0026gt;Server: Starts a request stream with an id of 4 Client-\u0026gt;\u0026gt;Client's encoder: encode the fields for \u0026lt;br/\u0026gt; the header fields in the request on stream 4 Client's encoder-\u0026gt;\u0026gt;Client: Encodes the fields by inserting the fields into \u0026lt;br/\u0026gt; dynamic table and add references \u0026lt;br/\u0026gt; to those entries for stream 4 Client's encoder-\u0026gt;\u0026gt;Server's decoder: Sends out Insert Instructions to update decoder's dynamic table par Client--\u0026gt;\u0026gt;Server: Sends out the HTTP request on stream 4 \u0026lt;br/\u0026gt; with encoded HEADERS frame Note right of Client: Client may use the entries \u0026lt;br/\u0026gt; before receiving the ack Server's decoder-\u0026gt;\u0026gt;Client's encoder: Sends out Insert Count Increment instruction to acknowledge its dynamic table update end Server-\u0026gt;\u0026gt;Server's decoder: Decodes the HEADERS \u0026lt;br/\u0026gt; frames on stream 4 Server's decoder-\u0026gt;\u0026gt;Client's encoder: Sends out Section Acknowledgment instruction \u0026lt;br/\u0026gt; after it decodes all fields in the received HEADER frame on stream 4 Client's encoder-\u0026gt;\u0026gt;Client's encoder: Clears the references \u0026lt;br/\u0026gt; associated with stream 4 end Client--\u0026gt;Server: Following client's requests on request streams \u0026lt;br/\u0026gt; of the same connection will repeat the processes above   References  HTTP/3 QPACK: Field Compression for HTTP/3 QUIC: A UDP-Based Multiplexed and Secure Transport ","date":"2023-03-28","permalink":"https://lianglouise.github.io/post/qpack_guide/","tags":["HTTP/3"],"title":"QPACK Guide"},{"content":"Apart from the linked list, there is another handful structure offered by Pintos kernel library - Hash Table.\nThe hash table in Pintos is not much different from any ordinary hash table. The technique it uses to solve hash collision is chaining which is the most classic solution.\nstruct hash_elem * hash_insert (struct hash *h, struct hash_elem *new) { struct list *bucket = find_bucket (h, new); struct hash_elem *old = find_elem (h, bucket, new); if (old == NULL) insert_elem (h, bucket, new); rehash (h); return old; } /* Inserts E into BUCKET (in hash table H). */ static void insert_elem (struct hash *h, struct list *bucket, struct hash_elem *e) { h-\u0026gt;elem_cnt++; /* New element will put at the front the list */ list_push_front (bucket, \u0026amp;e-\u0026gt;list_elem); }   This is the code used to insert a new element into the table. Note that the bucket is implemented in Linked List, so it can grow or shrink easily.\n Create A Hash Table To create a table, we need first declare a struct hash variable which will contains following:\n/* Hash table. */ struct hash { size_t elem_cnt; /* Number of elements in table. */ size_t bucket_cnt; /* Number of buckets, a power of 2. */ struct list *buckets; /* Array of `bucket_cnt' lists. */ hash_hash_func *hash; /* Hash function. */ hash_less_func *less; /* Comparison function. */ void *aux; /* Auxiliary data for `hash' and `less'. */ };  It is basically the same when you create a hash table in any other languages, requiring a hash function, comparing functions and it will store the pointers to each bucket.\nPut An Element Into Table And for the same reason described in the linked list post, to be able to put a struct into the table, developer has to add struct hash_elem as a member. Note that the struct hash_elem is just a wrap of struct list_elem since when doing inserting, our final operation is to put the element into the chaining linked list.\nstruct page { struct hash_elem hash_elem; /* Hash table element. */ void *addr; /* Virtual address. */ /* ...other members... */ };  Hash Functions Since C itself has no support for hash table, so it\u0026rsquo;s up developers to decide how to hash the data they want to put into the table. Luckily, Pintos\u0026rsquo;s authors have provided hash functions for most commonly used data types:\n// hash.h /* Sample hash functions. */ unsigned hash_bytes (const void *, size_t); unsigned hash_string (const char *); unsigned hash_int (int);  Comparing Functions Since we are using chaining to store the elements in the same bucket. When doing any operations, after locating the bucket, we need to iterate through the linked list to see if there exists the same element in the bucket. Thus we need also define a comparing function ourselves to check if they elements are different or not.\n/* Returns true if page a precedes page b. */ bool page_less (const struct hash_elem *a_, const struct hash_elem *b_, void *aux UNUSED) { const struct page *a = hash_entry (a_, struct page, hash_elem); const struct page *b = hash_entry (b_, struct page, hash_elem); return a-\u0026gt;addr \u0026lt; b-\u0026gt;addr; }   Costumed compared function in the hash table, here addr is the key of page hash table, so when we compares two pages, we can compare them by their addr values.\n Rehashing Since our bucket is implemented in Linked List, each time we do a look up would be $O(N)$ long, it will significantly decrease the look up speed if some hash collisions happening too frequently, i.e. some chaining becomes too long. We need to add more buckets from time to time so that more bytes would be used for hashing and we can have shorter chains (hopefully).\nIn terms of how to grow the number of buckets, the policy in Pintos is quite simple:\nstatic void rehash (struct hash *h) { // ... /* Calculate the number of buckets to use now. We want one bucket for about every BEST_ELEMS_PER_BUCKET. We must have at least four buckets, and the number of buckets must be a power of 2. */ new_bucket_cnt = h-\u0026gt;elem_cnt / BEST_ELEMS_PER_BUCKET; if (new_bucket_cnt \u0026lt; 4) new_bucket_cnt = 4; while (!is_power_of_2 (new_bucket_cnt)) new_bucket_cnt = turn_off_least_1bit (new_bucket_cnt); // ... }  Ideally, we hope the average number of elements in each bucket is 2 and no more than 4. So after inserting or deleting, the table will automatically recalculate the required bucket number and cast it down to the nearest power of two.\nAfter allocating the new memory space for the new buckets, it will read each element from the old buckets, rehash bytes based on the new bucket number and put the element into the corresponding new buckets.\n","date":"2022-03-21","permalink":"https://lianglouise.github.io/post/hash_table_in_pintos/","tags":["linux kernel","data structure"],"title":"Hash Table in Pintos"},{"content":"Abstract No matter for what kind of many screenplays, the most common topics among those watcher or fans are probably:\n Which xxx is the best? Which xxx do you recommend? Do you think what\u0026rsquo;s the best xxx of 2021?  xxx here can be substituted by any type of screenplays, i.e. movies, TV series, TV show and etc. This rule applies to one type of screenplays as well - TV anime.\nIn this post, I would present my analysis on what\u0026rsquo;s most welcomed TV animes in different period of time and what are the potential features make them become popular among viewers, with the help the data from anime community in Reddit and anime record data.\nBackground TV series in US usually have 23-24 episode as a \u0026ldquo;full season\u0026rdquo; and many of them run across the fall and winter, in between late September to May of the next year. However, unlike the conventions in North America, anime producers in Japan had very different traditions. The TV animes in Japan are usually played by seasons and last for three months, containing 12-13 episodes:\n Winter: January - March Spring: April - June Summer: July - September Fall: October - December  Therefore, in the project I would divide the animes in each year into four groups by natural seasons as the release time for the most episodes of an anime would fall into one of the four season, meaning if two animes belong to the same season, their release time of each episode would be very close\nBesides, the genres of the animes could very rich, covering a lot of topics and multiple themes, so we would be able to analyze if there is any potential connections between genres and the popularity.\nFinding the Hotest Anime in Different Period How to Extract the Data We Want? First of all, we need to determine what submissions are we actually want. Unlike IMDB or Rotten Tomatoes, there is no individual page in reddit for each anime so that people would only discuss or review that specific work under such page. The topics could be relatively spare and board. This can be also proved by a word cloud.\nfrom wordcloud import WordCloud import matplotlib.pyplot as plt wordcloud = WordCloud(width = 1000, height = 600, background_color=\u0026quot;white\u0026quot;, min_font_size = 16, font_step=2) wordcloud.generate(sub_titles['text'].str.cat(sep=' ')) plt.figure(figsize=(20, 10)) plt.imshow(wordcloud, interpolation='bilinear') plt.axis(\u0026quot;off\u0026quot;) plt.show()   Word cloud of title data among all submission in anime subreddit from text_submissions dataset\n From the word cloud we can see that, people discussed a lot topics in this subreddit, including but not limited to plots, characters they like, anime recommendations. It would be difficult for us to determine if they are talking about a specific anime and if they are talking about the animes currently on air or not by simply applying LatentDirichletAllocation from scikit-learn. As it\u0026rsquo;s very likely that the model will fail to extract the name of the anime properly.\nFortunately, The moderators of the anime subreddit and other contributors wrote a post bot4 which monitors the latest streaming info and will create a post automatically for each episode of anime after it\u0026rsquo;s released. And this is bot is currently operates under the account AutoLovepon.\n A typical discussion submission created by this bot\n Then we can simple go through the full data set we have, select the posts created by AutoLovepon, and use regex to extract anime title and episode number. Besides, since the full data set only contains meta data, I also used the praw library to help obtain the title name from reddit. As a result, we can acquire the data like below:\n anime: title of anime created_utc: date this submission created, which can be used to identify which season this anime belongs to com_mean, com_median, com_count: The mean, median of the scores of the comments under this submission and count the total comments score: The score of this submission  sta_19[sta_19['anime'] == \u0026quot;One Punch Man Season 2\u0026quot;].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  id created_utc anime season ep com_mean com_median com_count score     749 t3_bbatev 2019-04-09 17:34:10 One Punch Man Season 2 2 1.0 26.809337 2.0 2035.0 7756   784 t3_bdwte2 2019-04-16 17:40:48 One Punch Man Season 2 2 2.0 24.634706 2.0 1919.0 3743   830 t3_bgja8c 2019-04-23 17:39:02 One Punch Man Season 2 2 3.0 42.697674 3.0 1290.0 5468   881 t3_bj65ne 2019-04-30 17:37:40 One Punch Man Season 2 2 4.0 28.279570 3.0 930.0 3887   927 t3_bltood 2019-05-07 17:42:06 One Punch Man Season 2 2 5.0 22.986000 3.0 1000.0 2530     Rank Top 5 Animes of each season The score of each discussion submission the most obvious data we can use to compare. Thus, we can calculate the mean of each anime\u0026rsquo;s discussion submission\u0026rsquo;s mean and rank them.\nimport plotly.express as px top5_by_score = mean_scores.groupby('season').apply(lambda x: x.sort_values(by='score', ascending=False, na_position='first').head(5).reset_index()).droplevel(0) fig = px.line( top5_by_score, x=top5_by_score.index, y='score', color='season', symbol='season', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;score\u0026quot;: \u0026quot;Mean of Submission Score\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) fig.show()  rank_2019 = rank_seasons(2019) rank_2019   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Kaguya-sama wa Kokurasetai: Tensai-tachi no Re... 7204.916667 1   1 Mob Psycho 100 Season 2 6884.000000 1   2 Yakusoku no Neverland 4324.083333 1   3 Tate no Yuusha no Nariagari 3800.400000 1   4 Tensei shitara Slime Datta Ken 3382.166667 1   0 Shingeki no Kyojin Season 3 10257.600000 2   1 Kimetsu no Yaiba 4872.259259 2   2 One Punch Man Season 2 3825.666667 2   3 Isekai Quartet 2550.916667 2   4 Hitori Bocchi no â—‹â—‹ Seikatsu 1643.750000 2   0 Dr. Stone 4207.291667 3   1 Vinland Saga 4027.791667 3   2 Enen no Shouboutai 2533.791667 3   3 Tsuujou Kougeki ga Zentai Kougeki de Ni-kai Ko... 1760.000000 3   4 Dungeon ni Deai o Motomeru no wa Machigatte Ir... 1641.583333 3   0 Boku no Hero Academia Season 4 4608.181818 4   1 Sword Art Online: Alicization - War of Underworld 2264.833333 4   2 Fate/Grand Order: Zettai Majuu Sensen Babylonia 2143.090909 4   3 Shinchou Yuusha: Kono Yuusha ga Ore Tueee Kuse... 2067.833333 4   4 Ore o Suki na no wa Omae Dake ka yo 1840.363636 4     rank_2020 = rank_seasons(2020) rank_2020   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Boku no Hero Academia Season 4 4224.571429 1   1 Fate/Grand Order: Zettai Majuu Sensen Babylonia 2351.200000 1   2 Eizouken ni wa Te wo Dasu na! 2117.833333 1   3 Itai no wa Iya nano de Bougyoryoku ni Kyokufur... 2000.166667 1   4 Haikyuu!! To the Top 1984.384615 1   0 Kaguya-sama wa Kokurasetai?: Tensai-tachi no R... 10105.000000 2   1 Kaguya-sama wa Kokurasetai?: Tensai-tachi no R... 9362.000000 2   2 Kami no Tou 8229.000000 2   3 Kami no Tou: Tower of God 8040.500000 2   4 Otome Game no Hametsu Flag shika Nai Akuyaku R... 3128.000000 2   0 Re:Zero kara Hajimeru Isekai Seikatsu Season 2 12289.615385 3   1 Yahari Ore no Seishun Love Comedy wa Machigatt... 6253.833333 3   2 The God of High School 4913.384615 3   3 Maou Gakuin no Futekigousha: Shijou Saikyou no... 3780.916667 3   4 Sword Art Online: Alicization - War of Underwo... 3347.000000 3   0 Shingeki no Kyojin: The Final Season 16821.500000 4   1 Jujutsu Kaisen 6458.266667 4   2 Haikyuu!! To the Top 2nd Season 4450.000000 4   3 Haikyuu!!: To the Top Part 2 2831.363636 4   4 Higurashi no Naku Koro ni [Reboot only thread] 2758.666667 4     rank_2021 = rank_seasons(2021) rank_2021   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Shingeki no Kyojin: The Final Season 18219.307692 1   1 Re:Zero kara Hajimeru Isekai Seikatsu Season 2... 12320.250000 1   2 Jujutsu Kaisen 10981.727273 1   3 Mushoku Tensei: Isekai Ittara Honki Dasu 8043.727273 1   4 Horimiya 6959.461538 1   0 86 EIGHTY-SIX 7757.090909 2   1 Vivy: Fluorite Eye's Song 5536.461538 2   2 Fumetsu no Anata e 5449.750000 2   3 Ijiranaide, Nagatoro-san 4158.750000 2   4 Hige wo Soru. Soshite Joshikousei wo Hirou. 3315.076923 2     Since we have both score of a submission and score of a comments, I ranked the animes with\n mean of each submission\u0026rsquo;s score  import plotly.express as px import plotly.io as pio pio.renderers.default='iframe' top5_by_score = mean_scores.groupby('season').apply(lambda x: x.sort_values(by='score', ascending=False, na_position='first').head(5).reset_index()).droplevel(0) fig_1 = px.line( top5_by_score, x=top5_by_score.index, y='score', color='season', symbol='season', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;score\u0026quot;: \u0026quot;Mean of Submission Score\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) fig_1.show()    mean of total comments' score under the same submission  sta_19.fillna(0, inplace=True) mean_com_count = sta_19.groupby('anime').agg({'com_count': 'mean', 'season': 'min'}) top5_by_com_count = mean_com_count.groupby('season').apply(lambda x: x.sort_values(by='com_count', ascending=False, na_position='first').reset_index().head(5)).droplevel(0) fig_2 = px.line( top5_by_com_count, x=top5_by_com_count.index, y='com_count', color='season', symbol='season', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;com_count\u0026quot;: \u0026quot;Mean of comment's count\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) fig_2.show()    median of total comments' score under the same submission  med_com_score = sta_19.groupby('anime').agg({'com_median': 'mean', 'season': 'min'}) top5_by_com_score_med = med_com_score.groupby('season').apply(lambda x: x.sort_values(by='com_median', ascending=False, na_position='first').reset_index().head(5)).droplevel(0) fig_3 = px.line( top5_by_com_score_med, x=top5_by_com_score_med.index, y='com_median', color='season', symbol='season', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;com_median\u0026quot;: \u0026quot;Median of comment's Score\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) fig_3.show()    total number of comments under the same submission  sta_19.fillna(0, inplace=True) mean_com_score = sta_19.groupby('anime').agg({'com_mean': 'mean', 'season': 'min'}) top5_by_com_mean = mean_com_score.groupby('season').apply(lambda x: x.sort_values(by='com_mean', ascending=False, na_position='first').reset_index().head(5)).droplevel(0) fig_4 = px.line( top5_by_com_mean, x=top5_by_com_mean.index, y='com_mean', color='season', symbol='season', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;com_mean\u0026quot;: \u0026quot;Mean of comment's Score\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) fig_4.to_html(\u0026quot;\u0026quot;) fig_4.show()   So we now can see that Each of these metric would produce a relatively different result. In order to test if we should keep both of them in our ranking model as metric, I tested if they are relevant, particularly mean of submission\u0026rsquo;s score and mean of comment\u0026rsquo;s score.\nsta_19['comment_total'] = sta_19['com_mean'] * sta_19['com_count'] sta_19.plot.scatter(x='com_mean', y='score', logy=True, logx=True) sta_19.plot.scatter(x='comment_total', y='score', logy=True, logx=True)  \u0026lt;AxesSubplot:xlabel='comment_total', ylabel='score'\u0026gt;  Apparently, both the mean of and the sum of the comments' score are highly relevant to submission\u0026rsquo;s score, thus we only need to rely on the score of the submission to rank the animes. Then we can rank the anime in each season from 2019 to 2021 using the mean score of the discussion submission post.\nNow we can rank the top 5 animes in each season of each year as below\n 2019  rank_2019 = rank_seasons(2019) rank_2019   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Kaguya-sama wa Kokurasetai: Tensai-tachi no Re... 7204.916667 1   1 Mob Psycho 100 Season 2 6884.000000 1   2 Yakusoku no Neverland 4324.083333 1   3 Tate no Yuusha no Nariagari 3800.400000 1   4 Tensei shitara Slime Datta Ken 3382.166667 1   0 Shingeki no Kyojin Season 3 10257.600000 2   1 Kimetsu no Yaiba 4872.259259 2   2 One Punch Man Season 2 3825.666667 2   3 Isekai Quartet 2550.916667 2   4 Hitori Bocchi no â—‹â—‹ Seikatsu 1643.750000 2   0 Dr. Stone 4207.291667 3   1 Vinland Saga 4027.791667 3   2 Enen no Shouboutai 2533.791667 3   3 Tsuujou Kougeki ga Zentai Kougeki de Ni-kai Ko... 1760.000000 3   4 Dungeon ni Deai o Motomeru no wa Machigatte Ir... 1641.583333 3   0 Boku no Hero Academia Season 4 4608.181818 4   1 Sword Art Online: Alicization - War of Underworld 2264.833333 4   2 Fate/Grand Order: Zettai Majuu Sensen Babylonia 2143.090909 4   3 Shinchou Yuusha: Kono Yuusha ga Ore Tueee Kuse... 2067.833333 4   4 Ore o Suki na no wa Omae Dake ka yo 1840.363636 4      2020  rank_2020 = rank_seasons(2020) rank_2020   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Boku no Hero Academia Season 4 4224.571429 1   1 Fate/Grand Order: Zettai Majuu Sensen Babylonia 2351.200000 1   2 Eizouken ni wa Te wo Dasu na! 2117.833333 1   3 Itai no wa Iya nano de Bougyoryoku ni Kyokufur... 2000.166667 1   4 Haikyuu!! To the Top 1984.384615 1   0 Kaguya-sama wa Kokurasetai?: Tensai-tachi no R... 10105.000000 2   1 Kaguya-sama wa Kokurasetai?: Tensai-tachi no R... 9362.000000 2   2 Kami no Tou 8229.000000 2   3 Kami no Tou: Tower of God 8040.500000 2   4 Otome Game no Hametsu Flag shika Nai Akuyaku R... 3128.000000 2   0 Re:Zero kara Hajimeru Isekai Seikatsu Season 2 12289.615385 3   1 Yahari Ore no Seishun Love Comedy wa Machigatt... 6253.833333 3   2 The God of High School 4913.384615 3   3 Maou Gakuin no Futekigousha: Shijou Saikyou no... 3780.916667 3   4 Sword Art Online: Alicization - War of Underwo... 3347.000000 3   0 Shingeki no Kyojin: The Final Season 16821.500000 4   1 Jujutsu Kaisen 6458.266667 4   2 Haikyuu!! To the Top 2nd Season 4450.000000 4   3 Haikyuu!!: To the Top Part 2 2831.363636 4   4 Higurashi no Naku Koro ni [Reboot only thread] 2758.666667 4      2021 (First two season)  rank_2021 = rank_seasons(2021) rank_2021   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime score season     0 Shingeki no Kyojin: The Final Season 18219.307692 1   1 Re:Zero kara Hajimeru Isekai Seikatsu Season 2... 12320.250000 1   2 Jujutsu Kaisen 10981.727273 1   3 Mushoku Tensei: Isekai Ittara Honki Dasu 8043.727273 1   4 Horimiya 6959.461538 1   0 86 EIGHTY-SIX 7757.090909 2   1 Vivy: Fluorite Eye's Song 5536.461538 2   2 Fumetsu no Anata e 5449.750000 2   3 Ijiranaide, Nagatoro-san 4158.750000 2   4 Hige wo Soru. Soshite Joshikousei wo Hirou. 3315.076923 2     Furthermore, we then can generate the ranks in all three years we have in a facet plot which is categorized by year:\nrank_2019['year'] = 2019 rank_2020['year'] = 2020 rank_2021['year'] = 2021 total = rank_2019.append(rank_2020).append(rank_2021) fig = px.line( total, x=total.index, y='score', color='season', symbol='season', facet_col='year', hover_data=['anime'], labels={ \u0026quot;index\u0026quot;: \u0026quot;Rank\u0026quot;, \u0026quot;score\u0026quot;: \u0026quot;Mean of Submission Score\u0026quot;, \u0026quot;season\u0026quot;: \u0026quot;Season\u0026quot; }, ) sp = total[total['anime'].str.contains('Shingeki no Kyojin')] sp_data = px.scatter( sp, x=sp.index, y='score', text=\u0026quot;anime\u0026quot;, facet_col='year' ).update_traces(mode=\u0026quot;text\u0026quot;)[\u0026quot;data\u0026quot;] for trace in sp_data: fig.add_trace(trace) save_ploty(f\u0026quot;{Export_Path}/trend.html\u0026quot;, [fig]) fig.show()   Based on the facet plot above, over the years, the community is actually more and more active, as we can see that the mean of the score is increasing.\nAnd we can see that some series is extremely welcomed in fact, the season 3 and season 4 (which has 23 episodes) of Shingeki no Kyojin were released in season 2 2019, season 4 2020 and season 1 2021, The average score they received are significantly higher than the usual animes.\nTherefore, Apart from knowing which Anime is the hotest in it\u0026rsquo;s own season we can also see that Shingeki no Kyojin is the most popular anime in the community.\nExplore What Would Make an Anime Hot Since in the full dataset, we were only given the meta data of each submission and comment and the text data set is only a small subset of the full dataset. We are unable to do much further analysis to detect what would make an anime popular.\nLuckily, during doing this project, I also encountered an interesting dataset anime-offline-database, which contains the tag of each anime so we can pick up the tag data from it.\nThe columns I care about are:\n title: The name of anime tags: list of tags of the anime  we can try look into what kind of theme or subject would make a anime popular, in other words, try to connect the presence of tag in the anime with the score it can and build a model to predict the score with tag data.\nMerge the Records The only difficulty is that name of the same anime would be slightly different in two dataset, making us hard to do the exact match. The common differences are:\n Lower cases vs. Upper cases Series numbering Translation Habits  Some examples are shown as below:\nmerged_table[merged_table['anime'] != merged_table['anime_title']][['anime', 'anime_title']].head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  anime anime_title     0 3D Kanojo: Real Girl Season 2 3D Kanojo: Real Girl 2nd Season   2 Africa no Salaryman Africa no Salaryman (TV)   4 Aikatsu Friends! Aikatsu on Parade!   8 Ani ni Tsukeru Kusuri wa Nai! Season 3 Ani ni Tsukeru Kusuri wa Nai! 3   14 BEM EMOMOMO     So I solved the problem with Python\u0026rsquo;s built-in difflib to get the closet match of anime name in two dataset, then try to merge the animes which have name presented in the search result.\ndef find_closet(x): # perform fuzzy search on the anime title from reddit dataset # with title in the anime-offline-database res = difflib.get_close_matches(x, anime_meta_year['title'], cutoff=0.4) if len(res) == 0: return np.nan return res[0]  As a result, only 50 animes were dropped due to no matchin, which is pretty good given the orginal total number of animes in the reddit is 537.\nRegression With Linear Model After merging the tables, by applying explode() and pivot(), we can obtain a tag matrix, where columns are the presented tags and index are the anime names.\ntag_matrix = tags.pivot('anime', columns='tags', values='present').fillna(0) tag_matrix = tag_matrix.sort_index(axis=1, key=lambda x: tag_matrix[x].sum(), ascending=False) tag_matrix.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n tags comedy action drama fantasy slice of life present based on a manga male protagonist female protagonist shounen ... shounen ai soccer shounen-ai flat chested shrine maiden flash animation skateboarding slow when it comes to love fake romance exhibitionism   anime                          100-man no Inochi no Ue ni Ore wa Tatte Iru 1.0 1.0 1.0 1.0 0.0 0.0 1.0 1.0 0.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   2.43: Seiin Koukou Danshi Volley-bu 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   22/7 1.0 0.0 1.0 0.0 1.0 1.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   3D Kanojo: Real Girl Season 2 1.0 0.0 1.0 0.0 1.0 1.0 1.0 1.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0   A3! Season Autumn \u0026amp; Winter 1.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0    5 rows Ã— 717 columns\n In this case, we would explore with the Linear and Logistic Regressions in the linear model. We can split data into three parts - 70% as training data, 15% as test data and 15 % as validation data.\nfrom sklearn.linear_model import LinearRegression, LogisticRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split def rating_test(start, end, useLinear=True, step=1): test_mses = [] vali_mses = [] if useLinear: Model = LinearRegression else: Model = LogisticRegression for i in range(start, end, step): reg = Model().fit(train_set.iloc[:, 0:i], train_y) test_mses.append(mean_squared_error(test_y, reg.predict(test_set.iloc[:, 0:i]))) vali_mses.append(mean_squared_error(vali_y, reg.predict(vali_set.iloc[:, 0:i]))) test_mses = pd.Series(test_mses, index=[i for i in range(start, end, step)], name='Training MSE').rename_axis('n_features') vali_mses = pd.Series(vali_mses, index=[i for i in range(start, end, step)], name='Validation MSE').rename_axis('n_features') return test_mses, vali_mses test_mses, vali_mses = rating_test(1, 30) test_mses.plot.line(grid=True, marker='o', legend=True) vali_mses.plot.line(grid=True, marker='^', legend=True)  \u0026lt;AxesSubplot:xlabel='n_features'\u0026gt;  From the plot of MSE between predicted y and actual y is over $10^6$, so apparently, Linear Regression won\u0026rsquo;t work well with this data set.\nNow, let\u0026rsquo;s test with Logistic Regression\ntest_mses, vali_mses = rating_test(1, 30, False) test_mses.plot.line(grid=True, marker='o', legend=True) vali_mses.plot.line(grid=True, marker='^', legend=True)  \u0026lt;AxesSubplot:xlabel='n_features'\u0026gt;  Similarly, Logistic Model doesn\u0026rsquo;t work well on this dataset.\nTherefore, we cannot use the simple classifiers to predict scores of an anime.\nConclusion With given reddit dataset, it\u0026rsquo;s possible for us to determine what are the hottest animes over different period of time and find out how to rank the top n animes for simple recommendation system. But there are still some limitations using this dataset, such as detect what would be the worst anime, since the score mostly reflects how people are happy to discuss this submission but we are unable to tell what\u0026rsquo;s the attitude of people towards to a TV series. For example, if the quality of an episode is extremely, users might go to the submission and leave comments like analyzing why it\u0026rsquo;s bad and compilations about the plots which may still receive a very good score as a result. It might be a good idea to combine with sentiment score of the texts in the thread to detect if people are complaining, praising or just don\u0026rsquo;t care about the episode.\nI also attempted to use the linear model to find potential connection between the tags of animes and mean score of anime episode\u0026rsquo;s discussion submission. However, it turns out these two models didn\u0026rsquo;t work well given data, thus it may require some more powerful models to test if we can find some connections.\n","date":"2021-12-05","permalink":"https://lianglouise.github.io/post/d25_data_story/","tags":[],"title":"What's The Hottest Anime in Each Season?"},{"content":"This post is about an important data structure, embedded linked list, which is used massively in the pintos project amd offers a different idea how to create dynamic list in C. But its design usually made it easy to confuse the people at first, so it\u0026rsquo;s very interesting to look into its implementation.\nPintos is a simple Linux OS for the course CS140. It\u0026rsquo;s designed to help understand some important functions of OS, including threads, virtual memory and file system. Though it\u0026rsquo;s very simple compared to the real linux kernel, it still can run an real x86 simulator.\nSome Background Since Standard C doesn\u0026rsquo;t provide dynamic list library, it\u0026rsquo;s up to developers to decide how to implement the list when they need to use it. A very simple list, linked list, would be like:\nstruct node { struct foo *data_ptr; // save pointer to the data struct node *next; // save pointer to the next node in the list };  However, the biggest drawback of this implementation is that, we cannot reuse this list implementation and its library functions for other struct. If we want to create a reusable list library, we may have to introduce generics. For example, in C++, we can use #include \u0026lt;list\u0026gt; to import a list from STL library for any data types. The list node in the list is defined as below, which a double linked list using generics to allow it any kind of data.\n// stl_list.h struct _List_node_base { _List_node_base* _M_next; _List_node_base* _M_prev; // ... }; /// An actual node in the list. template\u0026lt;typename _Tp\u0026gt; struct _List_node : public __detail::_List_node_base { // ... _Tp _M_data; // ... };  But the problem is there is no generics in standard C. So instead of putting data into the list node, C developers come up with the idea of putting list node into the data struct.\nEmbedded Linked List Below is the core definitions of double linked list in the Pintos kernel. Some library functions are omitted.\n// src/lib/kernel/list.h struct list_elem { struct list_elem *prev; /* Previous list element. */ struct list_elem *next; /* Next list element. */ }; /* List. */ struct list { struct list_elem head; /* List head. */ struct list_elem tail; /* List tail. */ }; /* Converts pointer to list element LIST_ELEM into a pointer to the structure that LIST_ELEM is embedded inside. */ #define list_entry(LIST_ELEM, STRUCT, MEMBER) \\ ((STRUCT *) ((uint8_t *) \u0026amp;(LIST_ELEM)-\u0026gt;next \\ - offsetof (STRUCT, MEMBER.next))) void list_init (struct list *); /* List traversal. */ struct list_elem *list_begin (struct list *); struct list_elem *list_next (struct list_elem *); struct list_elem *list_end (struct list *);  Any data structures would potentially inserted into the list will need include a list_elem in its struct definition. For example, in this struct foo, we may want to put it into a list, then we need to add one struct list_elem into the struct foo.\nstruct foo { int data; struct list_elem elem; };  After we create a new foo struct using malloc(), we then can append it into the list without needing create a new node using malloc() again.\nstatic struct list g_list; int main () { list_init (\u0026amp;g_list); struct foo *e; // Create 10 nodes for (size_t i = 0; i \u0026lt; 10; i++) { e = malloc (sizeof (struct foo)); e-\u0026gt;data = i; list_push_back (\u0026amp;g_list, \u0026amp;e-\u0026gt;elem); } struct list_elem *k; for (k = list_begin (\u0026amp;g_list); k != list_end (\u0026amp;g_list); k = list_next (k)) { struct foo *f = list_entry (k, struct foo, elem); offsetof (struct foo, elem); printf (\u0026quot;%d\\n\u0026quot;, f-\u0026gt;data); } return 0; }  So from the code above, we can see that, this design makes it possible that, with only extra two pointers in each data struct, we can create generic list structure like c++.\nThe list can be also illustrated as\nwhere each struct list_elem elem inside has the addresses of previous and next struct list_elem elem. Thus, how it implements the traverse can basically be the same as any ordinary linked list, reading list_elem-\u0026gt;next for the memory address of the next node.\nSo far, we just traverse using struct list_elem, but how can we obtain the real data\u0026rsquo;s pointer (e.g., struct foo)? The marco list_entry() helps us to converts from the pointer to list_elem to the pointer to the actual data struct.\n// src/lib/kernel/list.h /* Converts pointer to list element LIST_ELEM into a pointer to the structure that LIST_ELEM is embedded inside. */ #define list_entry(LIST_ELEM, STRUCT, MEMBER) \\ ((STRUCT *) ((uint8_t *) \u0026amp;(LIST_ELEM)-\u0026gt;next \\ - offsetof (STRUCT, MEMBER.next))) // src/lib/stddef.h #define offsetof(TYPE, MEMBER) ((size_t) \u0026amp;((TYPE *) 0)-\u0026gt;MEMBER)  The marco does the work in a quiet hack way. Since we have the pointer to the struct list_elem and the layout of the data struct is fixed after compiling. Then, we could calculate the memory address offset between STRUCT and the MEMBER.next. Finally, move the pointer by decreasing the length of offset and we have the pointer to the address of data struct.\nIn terms of how we calculate the offset, as defined in another marco offsetof(), we can create a dummy data struct starting at memory address of 0 and try to obtain the pointer to the member of the data struct, then the address of that member would be offset we want. And since we don\u0026rsquo;t dereference the pointer, most compilers won\u0026rsquo;t complain even though 0 is an invalid address.\nThe reason why list_entry() uses list_elem.next instead of list_elem itself, I believe it\u0026rsquo;s that if we accidentally pass a member which is not type of list_elem then we may get a wrong offset. Using list_elem.next would allow the compiler to prevent the problem. But certainly, if that member struct also has a member named next, the complier won\u0026rsquo;t help.\nIn the Real Linux Kernel I also went into the code of the real linux kernel, and found a similar implementation in the linux/list.h and linux/types.h\n// include/linux/types.h struct list_head { struct list_head *next, *prev; }; // include/linux/list.h /** * list_entry - get the struct for this entry * @ptr:\tthe \u0026amp;struct list_head pointer. * @type:\tthe type of the struct this is embedded in. * @member:\tthe name of the list_head within the struct. */ #define list_entry(ptr, type, member) \\ container_of(ptr, type, member)  // include/linux/kernel.h /** * container_of - cast a member of a structure out to the containing structure * @ptr:\tthe pointer to the member. * @type:\tthe type of the container struct this is embedded in. * @member:\tthe name of the member within the struct. * */ #define container_of(ptr, type, member) \\ ({ \\ void *__mptr = (void *)(ptr); \\ BUILD_BUG_ON_MSG (!__same_type (*(ptr), ((type *)0)-\u0026gt;member) \\ \u0026amp;\u0026amp; !__same_type (*(ptr), void), \\ \u0026quot;pointer type mismatch in container_of()\u0026quot;); \\ ((type *)(__mptr - offsetof (type, member))); \\ })  From the source code of linux kernel, we can see that container_of() is very close to list_entry() in pintos but has some extra data type checking included.\nReferences  Pintos Project Linux Kernel Mirror ","date":"2021-07-30","permalink":"https://lianglouise.github.io/post/list_in_pintos/","tags":["linux kernel","data structure"],"title":"List in Pintos"},{"content":"This post is about part A of Lab3 of MIT 6.824, Distributed Systems. For previous two parts, please refer to the posts about the Lab 1 and 2 under the tag Distributed System. In this post, we are to build a fault tolerant key-value services on top of Raft.\n mermaid.initialize({startOnLoad:true, theme: \"neutral\" }); Overview The functions we need to achieve are in fact very simple. It will need three functions:\n Put(key, value): Create a new key-value pair. Replace with the vlaue if the keys exists in the map; Append(key, value): If the key exists in the map, append value to the end of old value. If not, do as what Put() does; Get(key): Return the value if key exists in the map. Otherwise, return an empty string.  If it\u0026rsquo;s a singe server app, it won\u0026rsquo;t take much time to implement. However, for fault tolerance, we will have to maintain multiple kv servers so that the service is available. Thus the real problem now is how to coordinate and synchronize the data between the servers through Raft.\nAs diagram illustrated above, we will have a server cluster, where raft service can see each other to do the election and log replications but the kv server can only communicate with its only Raft service in the cluster. Clients will randomly send requests to any kv servers. However, only the kv server whose raft is the leader will proceed the request and send back the data. All other kv servers will reject the request and ask the client to try other kv server.\nClient Side Let us begin with something easy first. The job of client is very simple, sending requests to one kv server, waiting for response. If timeout or receive any error, retry with other kv server.\nAs we will have multiple clients running parallelly, to make sure it\u0026rsquo;s able to know which client sent the request. We will assign each client an ID generated by nrand(). And there is also a chance that the same request might be sent multiple times. To identify the duplicate requests, each request should have an ID generated by nrand(), so that kv server is able to recognize the duplicate requests.\nBesides, to speed up the later requests, the client can cache the last seen leader ID (which is basically the index of server in ck.servers), so it doesn\u0026rsquo;t have to try from the first server.\nThus we can define the client as below:\ntype Clerk struct { servers []*labrpc.ClientEnd serverNum int clientId int64 // client's own uid lastLeaderId int // The leader id last heard }  When clients need to send the Get or PutAppend request, we can have Get defined like below as an example:\n// common.go type GetArgs struct { MsgId int64 ClientId int64 Key string } type GetReply struct { Key string Value string Err string } // client.go func (ck *Clerk) Get(key string) string { var reply GetReply args := GetArgs{ MsgId: nrand(), ClientId: ck.clientId, Key: key, } // Send Get Rpc and wait for the response // If timeout or error wrong leader, try anoterh server }  Server Side In the entire server cluster, only the server with the leader of Raft services can procced the requests from clients and send back the actual data to client. All other servers acts as back-up, only proceeding the request sent from the channel kv.applyCh and rejecting immediately any requests if finding itself not a leader in Raft services after calling rf.Start()\nThose we can illustrate the logic of the cluster as below:\ngraph TB client subgraph cluster kv1[KV Leader] kv2[KV Follower 1] kv3[KV Follower 2] rf_l([\"Raft Leader\"]) rf_1([\"Raft Follower 1\"]) rf_2([\"Raft Follower 2\"]) end client -- | a1. New request| kv1 kv1 -- |a2. Ask to log the requst| rf_l rf_l -- |a3. Append the new log| rf_1 \u0026 rf_2 rf_l -- |a4. complete the request from apply | kv1 rf_1 -- |a5. Send backed up load to its kv server | kv2 rf_2 -- |\"as (a5)\"| kv3 kv1 -- |\"a6. Success (with data return)\"| client client -. b1. New request .- kv2 kv2 -. b2. Falied due to not leader .- client   In section a, the request is sent to leader and got proceeded in the all the nodes in the cluster but in section b, the request is rejected as KV Follower 1 is not the leader in the cluster.\n Besides, another requirement is that duplicate requests must be omitted. Thus for each client, we need to cache the last request id it made.\nThus we can have the struct KVServer defined as:\ntype KVServer struct { mu sync.Mutex me int dead int32 // set by Kill() // Raft Server rf *raft.Raft applyCh chan raft.ApplyMsg persister *raft.Persister // snapshot if log grows this big maxraftstate int // sotre the key/value pairs in a map data map[string]string // keep a chan for each request submitted to raft. Delete if timeout or it's done waitChs map[int64]chan ApplyReply // cache the last request invoked by the client lastApplies map[int64]int64 }  Leader Proceeding the Requests All the operation won\u0026rsquo;t be applied to kv.data directly after receiving the request. Leader KVServer will first submit the request to Raft by rf.Start() and make a wait channel in kv.waitChs for this new request, for channel we need a timer to ensure that the goroutine Get() and PutAppend() will stop if taking too long to execute the request and can know the result of the execution without requiring additional lock for reading/updating the data. All the KVServer will execute the operation until they got the committed log sent back by Raft through kv.applyCh and send the execution result through kv.waitChs the goroutine Get() and PutAppend().\nYou also need to define a new struct or add some new fields to ApplyMsg , so that KVServer can know request client id, request message id and other data help you identify duplications.\nThen we can have:\nfunc (kv *KVServer) start(args applyArg) (rep ApplyReply) { index, term, isLeader := kv.rf.Start(args) // Reject if not leader if !isLeader { return ApplyReply{Err: ErrWrongLeader} } kv.lock() // Create a channel to watch if raft has reached // agreement for this request ch := make(chan ApplyReply) kv.waitChs[args.MsgId] = ch kv.unlock() // Wait until either timeout or getting response // from WaitCh } // goroutine to monitor the kv.applyCh\tfunc (kv *KVServer) waitApplyChan() { for !kv.killed() { // Wait the new logged request from raft msg := \u0026lt;-kv.applyCh\tsubmittedReq := msg.Command // acquire the log as we need to update and read kv.data kv.lock() // Check if it's a duplicate msg have seen before // You have to check it here since, the followers // can only see new req through kv.applyCh // With the op value and key(/value) in the req, update your data // use the waitCh to tell KVServer to send back the response // status cide and retrived data to client if ch, ok := kv.waitChs[submittedReq.MsgId]; ok { ch \u0026lt;- ApplyReply{Err: err, Value: value} } kv.unlock() } }  Changes in Raft A very important thing to remember in this Lab3A is that KVServer will lose all the data once it dies. The way to recover to the previous state are two:\n Raft replays all the committed logs it has from 0 to rf.commitIndex; Use RPC InstallSnapshot, which we are to implement in the next lab.  Thus the easiest solution for recovering the data of kv is that rf.lastApplied will be reset to 0 once rebooting, which indicates that kv has no data at all. So once Raft needs to send new ApplyMsg to kv, we should send rf.log[rf.lastApplied+1:newCommitIndex+1] and mark all the logs whose index \\(\\leq\\) oldCommitIndex invalid data as tester has seen these logs already.\n","date":"2021-02-10","permalink":"https://lianglouise.github.io/post/6.824_lab3a/","tags":["Distributed System"],"title":"KV Raft - 6.824 Lab3A"},{"content":"This post is about part C of Lab2 of MIT 6.824, Distributed Systems. For previous two parts, please refer to Part A and B. In this one we are focusing on Persist in Raft. The implementation would mostly follow the figure 2 in the paper as well.\nPersist It\u0026rsquo;s inevitable to have some machines crashed or rebooted in the clusters. We need to persist some important state so that nodes can begin at the place it crashed and rejoin the cluster again.\nAs Persistent state on all servers mentioned in figure 2, there three state to persist:\n  currentTerm: This will avoid the node to vote a candidate with a smaller term\n  votedFor: this prevent a node from voting multiple times in an election\n  log[]: if the node has the majority of committed logs, we need to ensure the future leader can see them\n  Implementation in the Lab Following the comments in function persist() and readPersist() of raft.go, we can fill the functions.\n// // save Raft's persistent state to stable storage, // where it can later be retrieved after a crash and restart. // see paper's Figure 2 for a description of what should be persistent. // // Call it when lock acquried func (rf *Raft) persist() { w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.currTerm) e.Encode(rf.votedFor) e.Encode(rf.log) data := w.Bytes() rf.persister.SaveRaftState(data) } // // restore previously persisted state // func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) \u0026lt; 1 { // bootstrap without any state? return } // Your code here (2C). // Example: r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var currTerm int var votedFor int var log []LogEntry if d.Decode(\u0026amp;currTerm) != nil || d.Decode(\u0026amp;votedFor) != nil || d.Decode(\u0026amp;log) != nil { panic(\u0026quot;Miss states for node recovery\u0026quot;) } else { rf.currTerm = currTerm rf.votedFor = votedFor rf.log = log DPrintf(\u0026quot;[%v] reboot, Term %v\u0026quot;, rf.me, rf.currTerm) } }  persist() needs to be called once one of the three variables listed above gets updated. And readPersist() should be called when Make() gets called.\nOptimization Some tests still get failed sometimes, below are the optimization on top of implementation following figure 2 to omit the failures.\nLog disagreement Sometimes the test would fail because taking too long to reach an agreement. You can follow the instructions in the An aside on optimizations in the post Students' Guide to Raft mentioned in the handout. Add two extra variables in AppendEntriesRPC reply: ConflictTerm and ConflictIndex to speed up decrement in stead of decreasing one term per round.\nSo at leader side we have:\n// Check if the leader has a log at the ConflitTerm // and get the last entry in the term lastIdx := rf.searchLastEntryInTerm(reply.ConflictTerm) if firstIndex != -1 { // Assume the follower have the all the logs in that term // and some bad logs in term between ConflictTerm and leader's term // have such a term, try to append from the next term rf.nextIndex[i] = firstIndex + 1 } else { // That term doesn't exist, so try to check if can agree on the log at ConflictIndex rf.nextIndex[i] = reply.ConflictIndex }  And at the follower side:\nif args.PreLogIndex \u0026gt; lastIdx { // The expected PreLogIndex \u0026gt; actual last index on follower // Ask for decrement reply.IsSuccess = false reply.ConflictIndex = len(rf.log) reply.ConflictTerm = -1 } else if rf.log[args.PreLogIndex].Term != args.PreLogTerm { // Unable to reach agreement at the PreLogIndex. Ask for decrement reply.IsSuccess = false reply.ConflictTerm = rf.log[args.PreLogIndex].Term reply.ConflictIndex = rf.searchFirstEntryInTerm(args.PreLogIndex) } else { // Able to reach agreement at the PreLogIndex // ... }  commitIndex Persist An other failure I sometimes met in the test TestFigure82C is the apply error:\nTest (2C): Figure 8 ... 2021/01/06 23:22:04 apply error: commit index=5 server=4 3879934624216030722 != server=3 6543482455276445645 exit status 1  As I checked the log, I see the situation like what happened in figure 8 in paper. Given 3 nodes, s1, s2 and s3:\n  s1, s2 and s3 initialized. s1 becomes leader of term 1;\n  s1 receives 3 new commands{1, 2, 3}, sending to s2 and s3. And then these 3 logs are committed successfully;\n  s2 disconnects. it begins to increase the term number and request for votes. say it has reached term 3;\n  s1 crashes and s2 reconnects. Now s2 has a higher term and identical logs. s2 becomes the new leader of term 4;\n  s2 receives 1 new command{4}. Before it sends to s3, it disconnects;\n  s1 now reboots but r2 disconnects. s1 may wins the election again as it has the identical logs. Not s1 and s3 comes to term 5.\n  s1 receive 1 new command{5}. the command successfully got applied to s1 and s3 and got committed;\n  s1 crashes, s2 reconnects and s3 restarts. s2 may wins the election if it jumps to the term 5 and election timer times out before s1\u0026rsquo;s timer. Now s2 and s3 comes to term 5.\n  Since s3 restarts, it\u0026rsquo;s commitIndex gets reset to 0. So s2 will try to let s1 overwrite command{5} with command{4}. As now s1\u0026rsquo;s commitIndex is 0. It will commit the log at the same index again.\n  To solve this problem, when calling persist(), add rf.commitIndex as well. So the node is aware of how many logs have been committed after reboot. And it will reject AppendEntriesRPC request where args.PreLogIndex \u0026lt; rf.commitIndex, thinking it the same as the request with a smaller term number.\nUnreliable Network Besides, I sometimes have the failure where index out of range when trying to send AppendEntriesRPC and use rf.nextIndex[i] to slice the logs.\nTest (2C): Figure 8 (unreliable) ... panic: runtime error: slice bounds out of range [303:115] goroutine 17137 [running]: ~/6.824/src/raft.(*Raft).broadcastHeartbeat(0xc0001ce9a0, 0x59) ~/6.824/src/raft/raft_appendEntry.go:223 +0x617 created by ~/6.824/src/raft.(*Raft).setStateLeader ~/6.824/src/raft/raft.go:208 +0x156 exit status 2  And this error would only happen when having an unreliable network. When checking the log, I see it\u0026rsquo;s because rf.nextIndex[i] get increased multiple times while trying to send AppendEntriesRPC to follower. After review the code, I see:\nif reply.IsSuccess { rf.nextIndex[i] += len(args.Entries) }  This is the reason why this error showed up. The replies of multiple heartbeat request may comes together and all of them works. So the nextIndex got updated multiple times.\nThe easiest fix is using the values in the request directly to set instead of addition.\n// In case of unreliable network, running multiple times rf.nextIndex[i] = args.PrevLogIndex + len(args.Entries) + 1  References  Students' Guide to Raft ","date":"2021-01-07","permalink":"https://lianglouise.github.io/post/6.824_lab2c/","tags":["Distributed System"],"title":"Raft - 6.824 Lab2C"},{"content":"This post is about part A and B of Lab2 of MIT 6.824, Distributed Systems. It is about the implementation of Raft. Here in these two part, we only discuss the section up to the end of section 5 in the paper.\nBrief Idea In GFS or MapReduce, we have one single Master to manage the entire system. However, there is still a chance that the Master encounters some error or other issues. We than need a cluster to avoid this single point failure. And again, as in the distributed system, it\u0026rsquo;s inevitable to have some nodes failed. We need a way to keep the requests and results contestant across the cluster as recovery and always have one leader to take the requests from the users.\nRaft is a consensus algorithm for managing a replicated log. Here log refers to requests to be backed up in the cluster. Each node maintains a list of log of the same order. As long as morn than half of the nodes are alive in the network, the cluster can still be functional.\nThe system we are to build follows the rules illustrated as this chart in the paper:\nPart A: Election And State Transiting There are three different states in Raft: leader, follower and candidate. In a working Raft network, it is only allowed to have one leader to take the request and try to back up the request to the other nodes. And all other nodes become follower to receive log back up from leader and maintain a timer to start a leader election if not listening back from the leader after a certain time by converting itself to be an candidate. And Raft uses an monophonically increasing integer as term number to tell different leaders.\nFollowing the state chart above and RequestVoteRPC, we can see what we need to implement:\nâ€‹\t1) Initially, Set all the Raft struct to be follower. Each of them start a timer electionTimeOut of random duration. After timer time out, follower turns itself to be candidate, increase term by 1 and start a leader election;\nâ€‹\t2) candidate uses RequestVoteRPC to ask all the peers to vote; if winning the votes from the majority, turning itself to be new leader and start the go routine broadcastHeartbeat to tell all the peers they should reset the timer electionTimeOut and wait for the commands;\nâ€‹\t3) If unable to win the majority in the electionTimeOut, increase term by 1 and restart a leader election.\nThus, the Raft struct should be defined as:\nconst ( Follower = iota Leader Candidate ) type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer's state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer's persisted state me int // this peer's index into peers[] dead int32 // set by Kill() applyCh chan ApplyMsg state RafeState currTerm int votedFor int lastUpdate time.Time log []LogEntry commitIndex int lastApplied int // Leader use, reset for each term nextIndex []int // index of the next log entry to send to that server matchIndex []int // index of highest log entry known to be replicated on server // Candidate use voteCount int // Increase by 1 if revice a vote from a peer. }  A rule in the figure 2 that all the nodes must follow:\nâ€‹\tIf RPC request or response contains term T \u0026gt; currentTerm: set currentTerm = T, convert to follower (Â§5.1)\nSo that term number can keep up-to-date in the entire network and an out-dated leader can know it should jump to a higher term.\nElection Restrictions Besides, as the rules stated in the figure 2 of paper and section 5.4.1, we have several rules to implement as election restriction.\nFor candidate, it loses an election if one of the following happens:\nâ€‹\t1) It\u0026rsquo;s killed. rf.killed() == true;\nâ€‹\t2) Some RequestVoteRPCReply has a higher term number than it\u0026rsquo;s currTerm;\nâ€‹\t3) Receive AppendEntryRPC with a higher term number than it\u0026rsquo;s currTerm.\nFor follower, it rejects the RequestVoteRPC if one of the following happens:\nâ€‹\t1) the request from the candidate with a lower term;\nâ€‹\t2) the request from the candidate which comes later and belongs to the same term;\nâ€‹\t3) the last entry having a higher term;\nâ€‹\t4) the logs longer than candidate when having the same term number.\nVoteFor The voteFor should be carefully managed, as failing to follow the rule in the figure 2 would lead to unexpected election result, especially when the nodes need to use records to persisted record to reboot in the part C.\nIf the node is a candidate, set its voteFor = rf.me\nIf the node receives a RequestVoteRPC  and replies voteGranted == true, then it should set its voteFor = candidateId\nIf the node becomes a followers only because receiving a RPC request from a node in a higher term and update itself to a follower , then it should set its voteFor = -1\nPart B: Log Replication In this part, Raft struct will receive logs from the test by rf.Start() to reach agreement among the peers and the log replication is one of the most important part of Raft.\nThe logic is quite straightforward:\nâ€‹\t1) leader receive a new command from the user through rf.Start() and appends this new log to the end of the rf.log[];\nâ€‹\t2) leader can call peers' AppendEntryRPC immediately after receiving one new log but also wait until leader calls broadcastHeartbeat to send all the new logs together, which is a good way to avoid calling AppendEntryRPC too frequently and have concurrent update issues with rf.nextIndex[] and rf.matchIndex;\nâ€‹\t3) follower receives a AppendEntryRPCArgs, comparing rf.log[args.PreLogIndex].Term with args.PreLogTerm. If equaling, follower can reach a agreement up to args.PreLogIndex then append the new entries in the request:\nâ€‹\tappend(rf.log[:args.PreLogIndex+1], args.Entries...)\nâ€‹\tOtherwise, follower replies with false, asking leader to decrease the args.PreLogIndex recursively until finding some index where leader and follower can reach an agreement. follower should abort the logs between args.PreLogIndex and rf.lastApplied;\nâ€‹\t4) After appending new log entries successfully, leader updates index value of rf.matchIndex[peerIdx] and then update its commitIndex by choosing a N where a majority of rf.matchIndex[i] \u0026gt;= N and N \u0026gt; rf.commitIndex;\nâ€‹\t5) In the next broadcastHeartbeat, follower receives the leader\u0026rsquo;s new commitIndex in args.LeaderCommit. If args.LeaderCommit \u0026gt; rf.commitIndex, then follower should update its commitIndex to be:\nâ€‹\tmin(args.LeaderCommit, rf.lastApplied)\nâ€‹\t6) Both leader and follower need to send logs between old and new commit Index to the channel ApplyMsg for testing.\nWe also need to be careful about the logic of handler AppendEntries:\nfunc (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.mu.Lock() defer rf.mu.Unlock() // Update state as term number in the args if args.Term \u0026lt; rf.currTerm || (args.Term == rf.currTerm \u0026amp;\u0026amp; rf.state == Leader) { // If receving a log with outdated term number, rejecting directly // If appearing 2 leaders, rejecting reply.IsSuccess = false reply.Term = rf.currTerm return } else if args.Term \u0026gt; rf.currTerm { // See a higher term turn to follower of term rf.setStateFollower(args.Term, -1) } else { // If same term just refresh the update time rf.lastUpdate = time.Now() } // Only with a valid term number, it should be considered as hearing from the leader // Update rf.log[] if args.PreLogIndex \u0026gt; rf.lastApplied { // The expected PreLogIndex \u0026gt; actual last index on follower. Ask for decrement reply.IsSuccess = false } else if rf.log[args.PreLogIndex].Term == args.PreLogTerm { // Able to reach agreement at the PreLogIndex, concat rf.log reply.IsSuccess = true //... } else if rf.log[args.PreLogIndex].Term != args.PreLogTerm { // Unable to reach agreement at the PreLogIndex due to differetn term number. Ask for decrement reply.IsSuccess = false } // If success on agrement and args.LeaderCommit \u0026gt; rf.commitIndex, // update follower's commit index and send apply msg to tester return }  Tips   Use go routine to send RPC parallelly. Also use go routine to send ApplyMsg as after commitment, they are safe to leave them.\n  Split the raft.go into multiple go files by the RPC types, as for each RPC you may need one handler for follower and multiple functions for leader;\n  For node state transiting, a good idea is to use set methods to group all the variables you need to update for state change so you won\u0026rsquo;t miss some of them.\nUse changing to Follower as an example:\nfunc (rf *Raft) setStateFollower(term, voteFor int) { rf.currTerm = term rf.votedFor = voteFor // Update last time hear from the leader rf.state = Follower rf.lastUpdate = time.Now() // Start a new election timer go rf.electionTimeOut(term) }    Use sort.Slice() to help leader find new commitIndex;\n  Election and heartbeat timers are suggested to implement using for {} instead of time.NewTimer. I also pass in a term to the timer so that when using set methods to update node\u0026rsquo;s state, you don\u0026rsquo;t have to warry about if you need to restart a new timer or not.\nUse heartbeat as an example:\nconst ( HeartbeatInterval = 120 * time.Millisecond ) func (rf *Raft) broadcastHeartbeat(term int) { for !rf.killed() { rf.mu.Lock() // Stop sent heartbeats if not a leader or has jumped to another term if rf.state != Leader || rf.currTerm \u0026gt; term { rf.mu.Unlock() break } // Some other operations in between... // Send heartbeat to all other peers for i, _ := range rf.peers { if i == rf.me { continue } // You can construct AppendEntriesArgs here or in the function go rf.sendAppendEntries(i) } rf.mu.Unlock() time.Sleep(HeartbeatInterval) } }    Always check Raft struct\u0026rsquo;s state and return value of rf.killed() after acquiring the lock. Return the function of break the loop if state changed or being killed.\n  References  Raft Visualization Students' Guide to Raft ","date":"2021-01-02","permalink":"https://lianglouise.github.io/post/6.824_lab2ab/","tags":["Distributed System"],"title":"Raft - 6.824 Lab2A 2B"},{"content":"This is the first lab of MIT 6.824, Distributed Systems. It is about the implementation of MapReduce, a framework introduced by Google that can process huge volume of data parallelly by splitting a single task to multiple small ones and assigning them to a cluster of \u0026ldquo;cheap\u0026rdquo; machines. And by using this framework, also as mentioned in the paper, this lets programmer make use of distributed system without the need of experience of it.\nA very classic use case of this framework is counting words in a large file, which is also what we are to implement.\nConcept Behind the Scene The overview of this framework is illustrated as the Figure 1 in the paper. There will be one Master and many Worker.\nMaster will assign tasks to Worker to execute and monitor the progress. It receives m input files and will generate r output files.\nWorker mainly works on two things:\n During Map phase, each Worker reads one of m input files, apply it to user-defined Map() which returns some \u0026lt;key, value\u0026gt; pairs. Then save them into intermediate files. Usually, there will be m Map tasks in total. During Reudece phase, each Worker reads the \u0026lt;key, value\u0026gt; pairs in the corresponding intermediate files and apply them to Reduce() and save the result to output file. There will be r Reduce tasks in total.  Implementation in the Lab We are required to implement four major components in the framework: Task, RPC, Workerand Master .\nTask This is a Go struct sent back-and-forth between Master and Worker.\nMaster -\u0026gt; Worker: start a new task.\nWorker -\u0026gt; Master: report the task result.\ntype MapReduceTask struct { // 0: Map, 1: Reduce, 2: Exit, 3: Wait TaskType int // 0: Unassigned, 1: Assigned, 2: Finished Status int // Start Time TimeStamp time.Time // Index in the list of tasks Index int InputFiles []string OutputFile []string }  For Map task, it\u0026rsquo;s expected to have 1 file name in InputFiles and nReduce of file name in OutputFile.\nSimilarly, for Reduce task, it\u0026rsquo;s expected to have nReduce of file name in InputFiles and 1 file name in OutputFile.\nRPC Since Master and Worker are different processes, we are asked to use Remote procedure call to send request and response between Master and Worker.\nIn this lab we can create two RPC, one for requesting a new task and one for submitting a task:\ntype RequestTaskArgs struct {} type RequestTaskReply struct { NReduce int Task MapReduceTask } type SubmitTaskArgs struct { Task MapReduceTask } // Here Master is always available type SubmitTaskReply struct {}  Worker worker is kind of single thread. It keeps requesting new task, processing it, report it and exit when master sends signal to exit.\nfunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { for { args := RequestTaskArgs{} reply := RequestTaskReply{} res := call(\u0026quot;Master.RequestTask\u0026quot;, \u0026amp;args, \u0026amp;reply) if !res { break } switch reply.Task.TaskType { case MAP: doMap(\u0026amp;reply, mapf) case REDUCE: doReduce(\u0026amp;reply, reducef) case WAIT: time.Sleep(1 * time.Second) case EXIT: os.Exit(0) } } }  Master The responsibilities for master are:\n  assigning the tasks to different worker. And if some worker does not report task back after certain time (10 sec here), reassign the task to another worker;\n  monitoring the progress. If all the map tasks are done, worker should start to assign reduce tasks. When all the reduce tasks are done, master needs to tell the worker to exit;\n  validating the result. Only confirm the worker\u0026rsquo;s output files are valid when the task is completed and submitted.\n  So the Master should be like below\ntype Master struct { inputFiles []string nReduce int mapTasks []MapReduceTask reduceTasks []MapReduceTask // Increase by 1 when one mapTask done. The map Phase is done when mapDone == inputFiles mapDone int // Increase by 1 when one reduceTask done. The reduce Phase is done when reduceDone == nReduce reduceDone int // Each time allow one work to update mutex sync.Mutex }  and when initializing it:\nfunc MakeMaster(files []string, nReduce int) *Master { m := Master{ inputFiles: files, nReduce: nReduce, mapTasks: make([]mapReduceTask, len(files)), reduceTasks: make([]mapReduceTask, nReduce), mapDone: 0, reduceDone: 0, mutex: sync.Mutex{}, } // Fill each task in array mapTasks with the input file name and itialize other fileds // Collect arrray for InputFiles and OutputFile in two tasks array // ...... return \u0026amp;m }  Tips  worker sometimes fails when processing the task. It might happen to write result to output files halfway. To avoid those garbage output, worker should be designed to write to an temp file and only when the entire task gets submitted, master then marks them are valid output.  // Worker create a new temp output file tmpFile, err := ioutil.TempFile(\u0026quot;./\u0026quot;, \u0026quot;mr\u0026quot;) // Master rename them when recv a submit request for _, file := range reply.Task.OutputFile { // validName := // mapTask: mr-\u0026lt;mapTask_idx\u0026gt;-\u0026lt;reduceTask_idx\u0026gt; //\treduceTask: mr-out-\u0026lt;reduceTask_idx\u0026gt; err := os.Rename(tempOut, validName) if err != nil { panic(err) } }   During the map phase, worker should use ihash(key) % NReduce as the reduce task number to group the result \u0026lt;key, value\u0026gt; pair into the same array.\n  All the field names in the RPC Args and Reply should begin with capital letter or Go will not send them.\n ","date":"2020-12-23","permalink":"https://lianglouise.github.io/post/6.824_lab1_mapreduce/","tags":["Distributed System"],"title":"MapReduce - 6.824 Lab1"},{"content":"After doing a handshake using X3DH, Both users can authenticate each other and agree on a shared master secret.\nSo what\u0026rsquo;s next?\nThe easiest solution is do as TLS: client and server share client_write_{key, iv} and server_write_{key, iv} and use a per-record sequence number to encrypt and decrypt messages in the session. However, as said in the previous post, in the situation of instant messaging software, one side of client may be offline for a long time. As a result , there exist some risk that once the client_write_{key, iv} and server_write_{key, iv} are compromised (though it\u0026rsquo;s kind of hard by brute forcing), all the future messages will be transparent to Mallory.\nThus, it\u0026rsquo;s very important to introduce an algorithm that add extra entropy while don\u0026rsquo;t have handshake again to generate a new master secret (which is quite expensive). This is why Double Ratchet was designed.\nHere I am using Double Ratchet without header encrypted as example.\nThe Key Chain The most important thing in Double Ratchet is maintaining key chains with three properties:\n  Resilience: The output keys appear random to an adversary without knowledge of the KDF keys. This is true even if the adversary can control the KDF inputs.\n  Forward security: Output keys from the past appear random to an adversary who learns the KDF key at some point in time.\n  Break-in recovery: Future output keys appear random to an adversary who learns the KDF key at some point in time, provided that future inputs have added sufficient entropy.\n  And each user will need to maintain three key chains similar to TLS except the root key chain:\n  Root chain: generate new root key for new write/read key and iv once receiving some new messages;\n  Writing chain: generate new write key and iv so user can encrypt messages with different key and iv while no new responses back;\n  Reading chain: generate new read key and iv to corresponding to the write keys and ivs in sequences for decryption.\n  Here writing chain and reading chain are very similar. They are both symmetric chains as their value only depends on the output value from root chain and previous write/read key.\nThe reason to introduce root chain is that writing chain and reading chain are both symmetric, once Mallory managed to steal some KDF output, this is no forward secrecy. However, The root chain\u0026rsquo;s key value depends on the previous root key and a new key generated by DH exchange (e.g. X22519). This will ensure that there won\u0026rsquo;t be a too long writing chain or reading chain.\nAs shown in the picture above, the new root key and new write/read key will be generated once Bob send a new X25519 Public key in the header of a new message.\nState Storing It\u0026rsquo;s very important to keep track of chains so the program can tell use which key to encrypt or decrypt:\nFor each user we need keep a record like this:\n{ \u0026quot;bob\u0026quot;: { \u0026quot;RK\u0026quot;: \u0026quot;The latest rook key input material\u0026quot;, \u0026quot;DH_pair\u0026quot;: [\u0026quot;Alice's current private key\u0026quot;, \u0026quot;Alice's current public key\u0026quot;], \u0026quot;DH_p\u0026quot;: \u0026quot;Current DH remote public key(Bob)\u0026quot;, \u0026quot;CKs\u0026quot;: [\u0026quot;list of the latest write chain key in each round\u0026quot;], \u0026quot;CKr\u0026quot;: [\u0026quot;list of the latest read chain key in each round\u0026quot;], \u0026quot;PN\u0026quot;: \u0026quot;# of DH process to generate new root key\u0026quot; } }  It\u0026rsquo;s update to you decide, the max number of write key and read key to keep in the list\nNote: we just need save the latest root key as for previous rounds write key or read key has been generated. So does DH_pair.\nThen in code:\n# Continue in Class Client from previous def dr_state_initialize(self, user_name, RK, DH_pair, DH_p): self.dr_keys[user_name] = { \u0026quot;RK\u0026quot;: RK, \u0026quot;DH_pair\u0026quot;: DH_pair, \u0026quot;DH_p\u0026quot;: DH_p, \u0026quot;CKs\u0026quot;: [], \u0026quot;CKr\u0026quot;: [], \u0026quot;Ns\u0026quot;: 0, \u0026quot;Nr\u0026quot;: 0, \u0026quot;PN\u0026quot;: 0 }  Integration with X3DH Double Ratchet is usually integrated with X3DH. We can initialize user state with the result from X3DH.\n  Alice (X3DH Sender):\n DH_pair: The EK, ephemeral key pair; DH_p: remain blank, waiting for Bob\u0026rsquo;s first response in DR format.    Bob (X3DH Receiver):\n DH_pair: remain blank, generate when send first message in DR format; DH_p: The EK_pa from X3DH Hello message.    Core Functions Following are the core functions implemented following the recommendations from Signal.\nKey Generation functions The KDF in the chains create new keys and iv to move the ratchet.\n  Root Key KDF:\nCreate rk_input_material for next round root key ratchet and ck for this round.\nBoth 32 bytes long\n  from Crypto.Protocol.KDF import HKDF def KDF_RK(rk, dh_out): out = HKDF(dh_out, 64, rk, SHA256, 1) rk_input_material = out[:32] ck = out[32:] return rk_input_material, ck    Write/Read Chain Key KDF:\nCreate ck_input_material for next round and mk to encrypt/decrypt this round\u0026rsquo;s message.\nBoth are 32 bytes long.\nck_input_material: use constant b'\\x01' as message input;\nmk: use constant b'\\x02' as message input.\n  from Crypto.Protocol.KDF import HKDF def KDF_CK(ck): ck_input_material = HMAC.new(ck, digestmod=SHA256).update(b'\\x01').digest() mk = HMAC.new(ck, digestmod=SHA256).update(b'\\x02').digest() return ck_input_material, mk  Data Encryption To generate the ciphertext in ENCRYPT() we need use mk to create keys K by:\nK = HKDF(mk, 80, b'\\0' * 80, SHA256, 1)  then sign header||plaintext||dh_pub_b using HMAC.new(K[32:64], digestmod=SHA256)\nthen pad signature||plaintext with empty bytes to make it fit AES Block Size,\nthen encrypt padded signature||plaintext using AES.new(K[:32], AES.MODE_CBC, iv=K[64:])\nFinally concat ciphertext as:\niv||AES(K[:32], Sig(K[32:64], header||plaintext||dh_pub_b), plaintext).\nPayload Format We can use JSON to separate header and ciphertext:\n{ \u0026quot;header\u0026quot;: { \u0026quot;dh_p\u0026quot;: \u0026quot;\u0026lt;New DH public key\u0026gt;\u0026quot;, \u0026quot;pn\u0026quot;: \u0026quot;\u0026lt;# of DH process to generate new root key(Including generating this message)\u0026gt;\u0026quot;, \u0026quot;n\u0026quot;: \u0026quot;\u0026lt;# of Message sent (Including this)\u0026gt;\u0026quot; }, \u0026quot;ciphertext\u0026quot;: \u0026quot;\u0026lt;output of ENCRYPT(mk, plaintext, header)\u0026gt;\u0026quot; }  Ratchet Update For each user, the key chains updated in the conditions below:\n  Sending:\n  Generate a new root key as send chain root key:\n At the initial state, there is no DH_pair to use; After receiving a new DH_p in the message header.    Use the existing send chain key:\n Try to send a message when not receive a new DH_p yet.      Receiving:\n  Generate a new root key as read chain root key:\n When there is a different DH_p in message header and having a bigger PN    Use existing read chain key:\n When message header has a smaller PN than the local one      Fin Though Double Ratchet is quite complicated. But keep in mind that, this is like playing a ping-pong game, update the root key and stop using the latest write key chain only when the \u0026lsquo;ball\u0026rsquo; is back.\nCertainly, the double ratchet and X3DH can be more secure, compared to TLS. However, this still only protects the application layer. But it\u0026rsquo;s still a very good solution since almost every key is dynamic in a long term session.\n","date":"2020-11-10","permalink":"https://lianglouise.github.io/post/some_practice_on_implementing_signal_protocol_with_python_2/","tags":["web","security"],"title":"Some Practice on Implementing Signal Protocol With Python (2): Double Ratchet"},{"content":"Lately I was working on a Web Security Project and I came across this very interesting Web Messaging Encryption Protocol - Signal Protocol.\nWhen doing web communication encryption, the most common solution is using TLS, where basically, two sides handshake using Diffieâ€“Hellman key exchange through an insecure channel and communicate using symmetric encryption (i.e.: AES) based on the secret key SK derivated from DH key exchange. This solution can provide strong enough encryption when communicating between client and server.\nHowever, when it comes to the situation where client talks to client (E2E) and server just redirects message, especially instant messaging, there are two major security issues we need to solve:\n  The server itself may be not secure and we are unable to trust it to store the messages in clear or weak encryption;\n  If always relying on the same SK, this gives Mallory time to crack it out and every message would be able to be decrypted.\n  To achieve secure E2E communications, the signal protocol was thus introduced. It has two main fragments:\n  Extended Triple Diffie-Hellman(X3DH): This extended DH key exchange helps two clients establish a shared SK for future communications in an asynchronous way, only requiring two clients publish some keys to server;\n  Double Ratchet algorithm: This algorithm provides both forward secrecy and break-in recovery, which makes each message have different SK so there is no way to crack one SK and know everything.\n  The specifications from Signal Protocol provide a relatively high level overview but there are still some details not that clear and hard to understand. So I would like to talk about how I understand this protocol with some actual implementation in Python. In this post, I will focus on the X3DH part.\nX3DH Protocol How shared DH Value computed in X3DH is using X25519 Key exchange which is a Elliptic-curve Diffieâ€“Hellman. And I chose library cryptography, which is part of pyOpenSSL, to compute this X25519 Key exchange.\nThe basic workflow of establishing a session using X3DH is illustrated as below:\nPreparation At first, both clients need to generate required key pairs, publish key bundles to server and save the secrets so they are able to start the session.\nKey pairs to generate:\n  IK: Long-Term Identity Key (32 bytes both), which is an unique identifier for each client;\n  SPK: Signed PreKey (32 bytes both), a key pair will be revoked and re-generated every few days/weeks for sake of security. Alongside, SPK_sig: SPK public key\u0026rsquo;s signature, signed by IK secret key - SIG(IK_s, SPK_p);\n  OPK: One-time Off Key (32 bytes both), a key pair will be revoked once used for handshake. Usually, the client will generate multiple OPK pair and generate new one once server used up or needs more.\n  Then all these key pair\u0026rsquo;s public keys and SPK_sig will be sent to server.\nfrom cryptography.hazmat.primitives.asymmetric import x25519 from XEdDSA import sign class User(): def __init__(self, name, MAX_OPK_NUM): self.name = name self.IK_s = x25519.X25519PrivateKey.generate() self.IK_p = self.IK_s.public_key() self.SPK_s = x25519.X25519PrivateKey.generate() self.SPK_p = self.IK_s.public_key() self.SPK_sig = sign(IK_s, SPK_p) self.OKPs = [] self.OPKs_p = [] for i in range(MAX_OPK_NUM): sk = x25519.X25519PrivateKey.generate() pk = sk.public_key() self.OPKs_p.append(pk) self.OKPs.append((sk, pk)) # for later steps self.key_bundles = {} self.dr_keys= {} def publish(self): return { 'IK_p': self.IK_p, 'SPK_p': self.SPK_p, 'SPK_sig': self.SPK_sig, 'OPKs_p': self.OPKs_p }  Due to unable to find out how to do XEdDSA signature required by signal protocol while using cryptography, since each library has their different way to format their X25519 Keys, here I picked a mock function sign. You could refer to this blog post and RFC Standard to convert between Ed25519 and X25519\nEstablish the Session To actually establish the session, steps 3-5 in the diagram above will be carried out by Alice and Bob.\n First, Alice tries to send first message. Her client will ask server for Bob\u0026rsquo;s key bundle and generate a Ephemeral Key pair use only for this handshake:  # Continue in Class Client # Get key bundle from a server object def get_key_bundle(self, server, user_name): if user_name in self.key_bundles and user_name in self.dr_keys: print('Already stored ' + user_name + ' locally, no need handshake again') return False self.key_bundles[user_name] = server.get_key_bundle(user_name) return True def initial_handshake(self, server, user_name): if get_key_bundle(user_name): # Generate Ephemeral Key sk = x25519.X25519PrivateKey.generate() self.key_bundles[user_name]['EK_s'] = sk self.key_bundles[user_name]['EK_p'] = sk.public_key() return    Then, Alice\u0026rsquo;s client will compute Alice\u0026rsquo;s secret key SK, with\n  IK_sa Alice\u0026rsquo;s secret Identity key,\n  EK_pk Alice\u0026rsquo;s public Ephemeral Key,\n  IK_pb Bob\u0026rsquo;s public Identity Key,\n  SPK_pb Bob\u0026rsquo;s public Signed PreKey,\n  OPK_pb Bob\u0026rsquo;s public One-time Off key\n  getting 4 DH values and derivate a 32 bytes SK, where SK = HKDF(DH_1||DH_2||DH_3||DH_4)\nfrom Crypto.Protocol.KDF import HKDF from Crypto.Hash import SHA256 from XEdDSA import verify KDF_F = b'\\xff' * 32 KDF_LEN = 32 KDF_SALT = b'\\0' * KDF_LEN # Continue in Class Client def x3dh_KDF(key_material): km = KDF_F + key_material return HKDF(km, KDF_LEN, KDF_SALT, SHA256, 1) def generate_send_secret_key(self, user_name): key_bundle = self.key_bundles[user_name] DH_1 = self.IK_s.exchange(key_bundle['SPK_p']) DH_2 = key_bundle['EK_s'].exchange(key_bundle['IK_p']) DH_3 = key_bundle['EK_s'].exchange(key_bundle['SPK_p']) DH_4 = key_bundle['EK_s'].exchange(key_bundle['OPK_p']) if not verify(self.IK_s, key_bundle['SPK_sig']): print('Unable to verify Signed Prekey') return # create SK key_bundle['SK'] = x3dh_KDF(DH_1 + DH_2 + DH_3 + DH_4)  The HKDF here I pick is from pycryptodome. And note that X3DH requires HKDF function prepend 32 b'\\xff' bytes if curve is X25519, and 57 b'\\xff' bytes if curve is X448. The salt should a b'\\0' byte sequence of length equals to length of output key length. Hash functions are required to be a 256-bit or 512-bit function.\n  Then, Alice\u0026rsquo;s client will build the hello message to send out:\nThe format of the initial message:\nIK_pb||EK_pa||OPK_pb||n_0||t_0||AES(SK, SIG(IK_sa, IK_pa||EK_pa||OPK_pb||AD)||IK_pa||IK_pb||AD)\nand you could set the AD\u0026rsquo;s format like this:\n{ \u0026quot;from\u0026quot;: \u0026quot;alice\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;bob\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;\u0026lt;some greeting messages\u0026gt;\u0026quot; }  Implement in Python:\nfrom cryptography.hazmat.primitives import serialization from Crypto.Random import get_random_bytes from Crypto.Cipher import AES import json # Length definition for hello message encryption AES_N_LEN = 16 AES_TAG_LEN =16 # Continue in Class Client def dump_privatekey(private_key, to_str=True): private_key = private_key.private_bytes( encoding=serialization.Encoding.Raw, format=serialization.PrivateFormat.Raw, encryption_algorithm=serialization.NoEncryption() ) return private_key def dump_publickey(public_key): public_key = public_key.public_bytes( encoding=serialization.Encoding.Raw, format=serialization.PublicFormat.Raw ) return public_key def build_x3dh_hello(self, server, to, ad): # Binary additional data b_ad = (json.dumps({ 'from': self.name, 'to': to, 'message': ad })).encode('utf-8') key_bundle = self.key_bundles[to] # 64 byte signature key_comb = dump_publickey(self.IK_p) + dump_publickey(key_bundle['EK_p']) +dump_publickey(key_bundle['OPK_p']) signature = sign(self.IK_s, key_comb + b_ad) print(\u0026quot;Alice message signature: \u0026quot;, signature) print(\u0026quot;data: \u0026quot;, key_comb + b_ad) # 16 byte aes nonce nonce = get_random_bytes(AES_N_LEN) cipher = AES.new(key_bundle['SK'], AES.MODE_GCM, nonce=nonce, mac_len=AES_TAG_LEN) # 32 + 32 + len(ad) byte cipher text ciphertext, tag = cipher.encrypt_and_digest(signature + dump_publickey(self.IK_p) + dump_publickey(key_bundle['IK_p']) + b_ad) # initial message: (32 + 32 +32) + 16 + 16 + 64 + 32 + 32 + len(ad) message = key_comb + nonce + tag + ciphertext server.send(to, message) # For Double Ratchet self.initialize_dr_state(to, key_bundle['SK'], [key_bundle['EK_s'], key_bundle['EK_p']], \u0026quot;\u0026quot;)  AES refers AES256 in GCM Mode, n_0 and t_0 are nonce and tag, using SK as key, which is also from pycryptodome.\n  Finally, Bob receives first message, decrypts and verifies it:\nBob will also check out Alice\u0026rsquo;s key bundle from server and manipulate the hello message to compute his SK. And then decrypt the message and verify the signature in the plaintext for AEAD.\nThe verifications include:\n  verify public signed PreKey\u0026rsquo;s signature SPK_sig;\n  verify IK_pb and OPK_pb in the hello message and in the local db matches;\n  verify IK_pa in the hello message and in key bundles matches;\n  verify AD, the json object in the hello message has correct from and to.\n  EC_KEY_LEN = 32 # Continue in Class Client def recv_x3dh_hello_message(self, server): # receive the hello message sender, recv = server.get_message() self.get_key_bundle(server, sender) key_bundle = self.key_bundles[sender] IK_pa = recv[:EC_KEY_LEN] EK_pa = recv[EC_KEY_LEN:EC_KEY_LEN*2] OPK_pb = recv[EC_KEY_LEN*2:EC_KEY_LEN*3] nonce = recv[EC_KEY_LEN*3:EC_KEY_LEN*3+AES_N_LEN] tag = recv[EC_KEY_LEN*3+AES_N_LEN:EC_KEY_LEN*3+AES_N_LEN+AES_TAG_LEN] ciphertext = recv[EC_KEY_LEN*3+AES_N_LEN+AES_TAG_LEN:] # Verify if the key in hello message matches the key bundles from server if (IK_pa != key_bundle['IK_p']): print(\u0026quot;Key in hello message doesn't match key from server\u0026quot;) return # Verify Signed pre key from server if not verify(key_bundle['IK_p'], key_bundle['SPK_sig']): print('Unable to verify Signed Prekey') return sk = create_recv_secret_key(IK_pa, EK_pa, OPK_pb) print('bob sk: ', sk) if sk is None: return key_bundle['SK'] = sk message = x3dh_decrypt_and_verify(self, key_bundle, IK_pa, EK_pa, nonce, tag, ciphertext) # For Double Ratchet self.initialize_dr_state(sender, sk, [], EK_pa) # Get Ek_pa and plaintext ad return EK_pa, message def generate_recv_secret_key(self, IK_pa, EK_pa, OPK_pb)): # Find corresponding secret OPK secret key # And remove the pair from the list OPK_sb = self.search_OPK_lst(OPK_pb) if OPK_sb is None: return IK_pa = x25519.X25519PublicKey.from_public_bytes(IK_pa) EK_pa = x25519.X25519PublicKey.from_public_bytes(EK_pa) DH_1 = self.SPK_s.exchange(IK_pa) DH_2 = self.IK_s.exchange(EK_pa) DH_3 = self.SPK_s.exchange(EK_pa) DH_4 = OPK_sb.exchange(EK_pa) # create SK return x3dh_KDF(DH_1 + DH_2 + DH_3 +DH_4) def x3dh_decrypt_and_verify(self, key_bundle, IK_pa, EK_pa, nonce, tag, ciphertext): # Decrypt cipher text and verify cipher = AES.new(decodeB64Str(sk), AES.MODE_GCM, nonce=nonce, mac_len=AES_TAG_LEN) try: p_all = cipher.decrypt_and_verify(ciphertext, tag) except ValueError: print('Unable to verify/decrypt ciphertext') return except Exception as e: print('e') return # Byte format of plain text sign = p_all[:EC_SIGN_LEN] IK_pa_p = p_all[EC_SIGN_LEN:EC_SIGN_LEN+EC_KEY_LEN] IK_pb_p = p_all[EC_SIGN_LEN+EC_KEY_LEN:EC_SIGN_LEN+EC_KEY_LEN*2] ad = p_all[EC_SIGN_LEN+EC_KEY_LEN*2:] if (IK_pa != IK_pa_p and IK_pb != IK_pb_p): print(\u0026quot;Keys from header and ciphertext not match\u0026quot;) return if not verify(IK_pa], sign, IK_pa_p + EK_pa + OPK_pb + ad): print(\u0026quot;Unable to verify the message signature\u0026quot;) return print('Message: ', json.loads(ad)) return json.loads(ad)    Next Step After both Alice and Bob share a SK, X3DH can be marked as completed. Though this process is hard to be cracked out, we have to integrate it with Double Ratchet Algorithm to reach ultimate secure in application layer (Again, this only encrypts the request/response data but not IP/TCP or others). And I will talk about it in the next part.\nFor now, we just need to make sure what to keep for DR integration and what to destroy to avoid reuse attack.\nAlice, Bob and server need to destroy the OPK from Bob used in this handshake.\nAlice needs to keep EK pair and SK to initiate DR.\nBob also needs to keep Alice\u0026rsquo;s EK_p and SK to initiate DR and send his response using DR.\n","date":"2020-10-20","permalink":"https://lianglouise.github.io/post/some_practice_on_implementing_signal_protocol_with_python_1/","tags":["web","security"],"title":"Some Practice on Implementing Signal Protocol With Python (1): X3DH"},{"content":"Welcome to my github page.\nFinally! At the end of 2019, I managed to launched my personal blog with the help of Hexo and archer theme. with the help of Hugo and Fuji theme.\nI will share my naive thoughts and experiences on coding here. ðŸ˜œ\nHope you could find something helpful and interesting.\n","date":"2019-12-31","permalink":"https://lianglouise.github.io/post/hello_world/","tags":["Intro"],"title":"Hello-World!"}]